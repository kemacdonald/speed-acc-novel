---
title: "Integration of gaze information during online language  \n comprehension and learning"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Kyle MacDonald (kemacdonald@ucla.edu)} \\ Department of Communication, UCLA 
    \AND {\large \bf Elizabeth Swanson (elizswan@stanford.edu)} \\ Department of Psychology, Stanford University 
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, Stanford University 
    }

abstract: >
     Face-to-face communication provides access to visual information that can support language processing. But do listeners automatically seek social information without regard to the language processing task? Here, we present two eye-tracking studies that ask whether listeners' knowledge of word-object links changes how they actively gather a social cue to reference (eye gaze) during real-time language processing. First, when processing familiar words, children and adults did not delay their gaze shifts to seek a disambiguating gaze cue. When processing novel words, however, children and adults fixated longer on a speaker who provided a gaze cue, which led to an increase in looking to the named object and less looking to the other object in the scene. These results suggest that listeners use their knowledge of object labels when deciding how to allocate visual attention to social partners, which in turn changes the visual input to language processing mechanisms.
    
keywords: >
    eye movements; language processing; information-seeking; word learning; gaze following
    
output: cogsci2016::cogsci_paper
header-includes:
  - \usepackage{threeparttable}
  - \usepackage{booktabs}
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.asp=0.6, fig.align = 'center',
                      fig.crop = F, out.width = "80%",
                      fig.pos = "t", fig.path='figs/',
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)

source(here::here("code/helper_functions/libraries_and_functions.R"))
source(here::here("code/helper_functions/permutation_helpers.R"))
source(here::here("code/helper_functions/ewma_helper_funs.R"))
source(here::here("code/helper_functions/paper_helpers.R"))
bib <- knitcitations::read.bibtex(here::here("writing/manuscript/speed-acc-novel.bib"))

# data_paths
demo_path <- "data/01_participant_logs/"
data_path <- "data/03_processed_data/"
stimuli_path <- "data/00_stimuli_information/analysis_order_sheets"
image_path <- "writing/figures/plots"
```

```{r model-globals}
library(tidybayes)
options (mc.cores=parallel::detectCores()) # Run on multiple cores
set.seed (3875)
```

```{r read stimuli information}
d_gaze_stim <- read_csv(here::here(stimuli_path, "speed-acc-child-gaze-trial-info.csv"), col_types = cols(.default = "c"))
d_adult_ng_stim <- read_csv(here::here(stimuli_path, "speed-acc-adult-ng-trial-info.csv"), col_types = cols(.default = "c"))
d_stim <- bind_rows(mutate(d_gaze_stim, experiment = "kids_gaze"), mutate(d_adult_ng_stim, experiment = "adults_ng"))
```

```{r read_familiar_words_data}
d_fam_child <- read_csv(here::here(data_path, "familiar_words/speed_acc_child_gaze_fstshift_tidy.csv")) %>% 
  rename(age_days = age) %>% 
  mutate(age_group = "children")

d_fam_adult <- read_csv(here::here(data_path, "familiar_words/speed_acc_adult_ng_fstshift_tidy.csv")) %>% 
  filter(noise_condition == "clear") %>% 
  rename(run_date = dot,
         comments = notes) %>% 
  select(-age, -resp_onset_type_fact, -reason_excluded) %>% 
  mutate(birthday = NA,
         age_group = "adults", 
         age_days = NA,
         run_date = lubridate::as_datetime(run_date))

# clean up and join kid and adult data
d_fam <- bind_rows(d_fam_child, d_fam_adult)
```

```{r read_speed_novel_data}
d_novel_fs <- read_csv(here::here(data_path, "novel_words/speed_acc_novel_shifts.csv")) %>% 
  mutate(learn_block_half = ifelse(trial_num_learn_block <= 2, "first", "second"),
         trial_num_lblock_8 = case_when(
           trial_type == "exposure" & trial_num_learn_block == 1 ~ "1",
           trial_type == "test" & trial_num_learn_block == 1 ~ "2",
           trial_type == "exposure" & trial_num_learn_block == 2 ~ "3",
           trial_type == "test" & trial_num_learn_block == 2 ~ "4",
           trial_type == "exposure" & trial_num_learn_block == 3 ~ "5",
           trial_type == "test" & trial_num_learn_block == 3 ~ "6",
           trial_type == "exposure" & trial_num_learn_block == 4 ~ "7",
           trial_type == "test" & trial_num_learn_block == 4 ~ "8",
           TRUE ~ "NA"),
         correct_bool = ifelse(shift_accuracy == "correct", TRUE, FALSE),
         correct_num = ifelse(shift_accuracy == "correct", 1, 0),
         log_rt = log(rt),
         age_category = ifelse(age_category == "child", "children", "adults")) %>% 
  mutate(age_category = factor(age_category, levels = c("children", "adults")),
         gaze_on_trial = case_when(
           gaze_condition == "gaze" & trial_type == "exposure" ~ "gaze",
           TRUE ~ "no_gaze"
         )
  )

d_novel_tc <- read_feather(here::here(data_path, 
                                      "novel_words/speed_acc_novel_timecourse.feather")) %>% 
  mutate(learn_block_half = ifelse(trial_num_learn_block <= 2, "first", "second"),
         trial_num_lblock_8 = case_when(
           trial_type == "exposure" & trial_num_learn_block == 1 ~ "1",
           trial_type == "test" & trial_num_learn_block == 1 ~ "2",
           trial_type == "exposure" & trial_num_learn_block == 2 ~ "3",
           trial_type == "test" & trial_num_learn_block == 2 ~ "4",
           trial_type == "exposure" & trial_num_learn_block == 3 ~ "5",
           trial_type == "test" & trial_num_learn_block == 3 ~ "6",
           trial_type == "exposure" & trial_num_learn_block == 4 ~ "7",
           trial_type == "test" & trial_num_learn_block == 4 ~ "8",
           TRUE ~ "NA"
         ))
```

# Introduction

<!-- Research on lexical development has pursued several solutions.  Lab-based studies and computational models show that learners can overcome referential uncertainty within a labeling event by tracking the elements of a context that remain consistent across multiple exposures to a new word (cross-situational learning) [@yu2007rapid; @roy2002learning]. Social-pragmatic accounts highlight research showing that children's social partners reduce the complexity of the learning task by using gesture and eye gaze to coordinate language interactions with children [@estigarribia2007getting; @bloom2002children]. Moreover, even 16-month-olds can use cues from other people (e.g., the direction of their gaze) to infer the meaning of a new word [@baldwin1993infants]. -->

<!-- Both social and statistical information can reduce uncertainty about reference during language processing. These processes, however, do not operate in isolation, and a sophisticated learner might integrate information from both sources to learn words. Computational models show better learning by integrating social information with cross-situational learning mechanisms [@yu2007unified; @frank2009using]. -->

<!-- The statistical and social accounts of word learning reviewed above reflect a somewhat passive construal of the learner. Children, however, take actions to shape learning (e.g., choosing where to look or point). Empirical work shows that "active" control can speed learning because it allows people to integrate prior knowledge and uncertainty to seek more useful information [@gureckis2012self]. For example, @kachergis2013actively  found that adults who selected objects to be labeled learned more word-object associations compared to adults who passively experienced labels.  -->

Understanding language in real-time is hard. Consider that even in grounded language comprehension, people can talk about many things, with no guarantee that they use familiar words. This flexibility creates the potential for a speaker's intended meaning to be mostly unconstrained. Listeners, however, can overcome ambiguity by integrating visual information available in face-to-face communication (e.g., the direction of a speaker's gaze) that constrains the interpretation of an utterance [@vigliocco2014language]. But do listeners strategically seek supportive visual information from other speakers? And does this information seeking depend on the listener's uncertainty over word meaning? 

Prior empirical and theoretical work on language processing, shows that listeners integrate input from multiple sources to constrain the set of possible interpretations of an utterance (see McClelland, Mirman, and Holt, 2006, for a review). For example, adults are faster to process information and make fewer comprehension errors if a speaker provides gestures with redundant cues to meaning [@kelly2010two]. Moreover, developmental research shows that parents actively use visual cues such as gestures and direction of their gaze to structure language interactions with their children [@estigarribia2007getting]. And even 16-month-olds can use the direction of another speaker's gaze to infer the meaning of a new word [@baldwin1993infants].

These findings suggest that visual information can facilitate language processing. The visual signal, however, is transient and the value of fixating on a communicative partner can vary depending on the language processing task. Thus, rather than randomly fixate a scene, listeners might strategically deploy their fixations to informative locations that maximize successful comprehension. For example, in prior work, we found that children and adults fixated longer on a speaker's face when processing familiar words in a "noisy" auditory environment, suggesting that they compensated for uncertainty in the auditory signal by gathering more visual information [@macdonald2018children]. Moreover, recent theoretical and empirical work suggests that children and adults handle noise in the signal by integrating what they perceive with their prior beliefs about the speaker's intended meaning [@gibson2013rational; @yurovsky2017preschoolers; @fourtassiword2018].

Here, we pursue the idea that the value of fixating on another speaker could vary as a function of (1) the helpfulness of the interlocutor and (2) the listeners' prior experience with a word-object link. In two experiments, we manipulate whether a speaker provides a valid visual cue to reference: by directing her eye gaze to an object while either producing a concrete, familiar word ("ball") or a novel word ("dax"). We chose this behavior as a case study of active information seeking because gaze following is thought to be relevant for the ecological task of linking language to the world and recent work has found a reliable link between sustained visual attention on objects and word learning [@smith2013visual]. 

Our second goal is to test whether children and adults would show a similar pattern of adapting the timing of their gaze shifts to gather a social cue to reference. It could be that children rely more on visual information from social partners because they have partial knowledge of word-object links and are developing a language model. Moreover, adults have more prior experience with the familiar words in our study and stronger novel word learning skills. Both of these features could diminish the usefulness of seeking a social cue when they process familiar or novel words.

```{r gaze-stimuli, fig.env = "figure*", fig.pos = "h", fig.width=5, fig.asp = 0.3, out.width= "65%", fig.cap = "Stimuli for Experiments 1 and 2. Panel A shows the structure of the linguistic stimuli for a single trial. Panel B shows the layout of the fixation locations for all tasks: the center stimulus, the target, and the distracter. Panel C shows a sample of the images used as novel objects in Experiment 2. Panel D shows an example of the social gaze manipulation."}

knitr::include_graphics(here::here(image_path, "gaze_stimuli.jpeg"))
```

We characterize eye movements as a series of information seeking decisions that aim to minimize uncertainty about the external world [@hayhoe2005eye]. Under this account, listeners should consider the utility of fixating on a speaker for achieving their current task goal. We hypothesized that when a communicative partner produces a gaze cue, they become more valuable for the task of disambiguating reference. Our key behavioral prediction is that listeners will delay generating an eye movement away from a speaker's face until seeing where she directs her gaze. This delay will lead to an increase in fixations to the named object.

# Experiment 1

In Experiment 1, we measured the time course of children and adults' decisions about visual fixation as they processed sentences with familiar words ("Where's the ball?"). We manipulated whether the speaker produced a post-nominal gaze cue to the named object. The visual world consisted of three fixation targets (a center video of a person speaking and a target/distracter picture). The primary question of interest was whether listeners would delay shifting their gaze away from the speaker's face if she was likely to generate a gaze cue. We predicted that fixating longer on the speaker would allow listeners to gather more language-relevant visual information to facilitate comprehension. In contrast, if listeners show parallel gaze dynamics across conditions, this would suggest that hearing the familiar word was the primary factor driving shifts in visual attention.

```{r speed-acc-gaze-results, fig.env = "figure*", fig.pos = "t", fig.width=4, fig.asp = 0.4, out.width= "85%", fig.cap = "Timecourse looking, first shift Reaction Time (RT), and Accuracy results for children and adults in Experiment 1. Panel A shows the overall looking to the center, target, and distracter stimulus for each gaze condition and age group. Panel B shows the distribution of pairwise contrasts between each participant's RT in the gaze and no-gaze conditions. The square point represents the group means. The vertical dashed line represents the null model of zero condition difference. Error bars represent the 95\\% HDI. Panel C shows the same information but for first shift accuracy."}

knitr::include_graphics(here::here(image_path, "speed_acc_fam_behav.jpeg"))
```

## Analytic approach

To quantify evidence for our predictions, we present two analyses. First, we analyze the time course oflooking to each area of interest (AOI). This measure reflects the mean proportion of trials on which participants fixated on the speaker, the target, or the distracter at every 33-ms interval of the stimulus sentence. We tested condition differences in the proportion looking to the language source using a nonparametric cluster-based permutation analysis, which accounts for the issue of taking multiple comparisons across many time bins in the timecourse [@maris2007nonparametric]. A higher proportion of looking to the language source in the gaze condition would indicate listeners' prioritization of seeking visual information from the speaker.

Next, we analyzed the RT and Accuracy of participants' initial gaze shifts away from the speaker to the objects in the scene. RT corresponds to the latency of shifting gaze from the central stimulus to either object measured from the onset of the target noun. All RT distributions were trimmed to between zero and two seconds, and RTs were modeled in log space. Accuracy corresponds to whether participants' first gaze shift landed on the target or the distracter object. If listeners generate slower and more accurate shifts, this provides evidence that gathering more visual information from the speaker led to more robust language processing.

We used the `brms` [@burkner2017brms] package to fit Bayesian mixed-effects regression models. The mixed-effects approach allowed us to model the nested structure of our data -- multiple trials for each participant and item, and the within-participants manipulation. We used Bayesian estimation to quantify uncertainty in our point estimates, which we communicate using a 95% Highest Density Interval (HDI), providing a range of credible values given the data and model.

## Methods

### Participants

```{r exclusions-table}
# keep_et means too few trials
n_exclude_et_e1 <- d_fam %>% 
  select(subid, keep_et, age_group) %>% 
  unique() %>% 
  count(keep_et, age_group)

n_exclude_runsheet_e1 <- d_fam %>% 
  select(subid, keep_runsheet, age_group) %>% 
  mutate(keep_runsheet = ifelse(keep_runsheet == "keep" | keep_runsheet == "yes", "keep", "drop")) %>% 
  unique() %>% 
  count(keep_runsheet, age_group)

total_exc_children <- n_exclude_et_e1$n[2] + n_exclude_runsheet_e1$n[2]
total_exc_adults <- n_exclude_et_e1$n[1] + n_exclude_runsheet_e1$n[1]
```

```{r make-ss-table, results="asis"}
d_kids <- d_novel_fs %>% 
  filter(age_category == "children", keep_drop == "keep") %>% 
  select(subid, age_days, gender) %>% 
  mutate(Experiment = "Exp. 2 (novel words)") %>% 
  unique()

d_kids %<>% bind_rows(
  d_fam %>% 
    filter(age_group != "adult", keep_et == "include", keep_runsheet == "keep") %>% 
    select(subid, age_days, gender) %>% 
    unique() %>% 
    mutate(Experiment = "Exp. 1 (familiar words)" )
)

gender_breakdown <- d_kids %>% 
  count(Experiment, gender)

kids_age_table <- d_kids %>% 
  mutate(age_months = age_days / 30.25,
         age_years = age_days / 365.25) %>% 
  group_by(Experiment) %>% 
  summarise(n = n(),
            Mean = round(mean(age_months, na.rm = T), 1),
            Min = min(age_months, na.rm = T),
            Max = max(age_months, na.rm = T))

kids_age_table[, -1] <- papaja::printnum(kids_age_table[, -1])

apa_table(
  kids_age_table
  , caption = "Age distributions of children in Experiments 1 and 2. All ages are reported in months."
)
```

```{r adults-demo}
n_adults_gender <- d_novel_fs %>% 
  filter(age_category == "adults", keep_drop == "keep") %>% 
  select(subid, gender) %>% 
  unique() %>% 
  count(gender) %>% 
  mutate(Experiment = "Experiment 3")

n_adults_gender %<>% bind_rows(
  d_fam %>% 
    filter(age_group == "adults", keep_et == "include") %>% 
    select(subid, gender) %>% 
    unique() %>% 
    count(gender) %>% 
    mutate(Experiment = "Experiment 1")
)

n_adults_total <- n_adults_gender %>% group_by(Experiment) %>% summarise(n = sum(n))
```

Participants were native, monolingual English-learning children ($n=$ `r kids_age_table$n[1]`; `r gender_breakdown$n[1]` F) and adults ($n=$ `r n_adults_total$n[1]`; `r n_adults_gender$n[3]` F). All participants had no reported history of developmental or language delay and normal vision. `r total_exc_adults + total_exc_children` participants (`r total_exc_children` children, `r total_exc_adults` adults) were run but not included because either the eye tracker failed to calibrate (`r n_exclude_runsheet_e1$n[2]` children, `r n_exclude_runsheet_e1$n[1]` adults) or the participant did not complete the task (`r n_exclude_et_e1$n[2]` children, `r n_exclude_et_e1$n[1]` adults). 

### Materials

```{r linguistic stimuli length}
d_word_lengths <- d_stim %>% 
  filter(!is.na(noun)) %>% 
  select(noun, carrier, noun_onset_sec:noun_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("noun_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (noun_offset_sec * 33) + noun_offset_frames ) - ( (noun_onset_sec * 33) + (noun_onset_frames) ),
    length_ms = length_frames * 33
  ) 

ms_words_length <- d_word_lengths %>% 
  group_by(carrier, noun) %>% 
  summarise(m_length = mean(length_ms)) %>% 
  group_by(noun) %>% 
  summarise(m = mean(m_length) %>% round(digits = 0)) 

## compute gaze length
d_gaze_length <- d_stim %>% 
  filter(!is.na(gaze_onset_sec)) %>% 
  select(noun, carrier, gaze_onset_sec:gaze_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("gaze_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (gaze_offset_sec * 33) + gaze_offset_frames ) - ( (gaze_onset_sec * 33) + (gaze_onset_frames) ),
    gaze_length_ms = length_frames * 33
  ) 

ms_gaze_length <- d_gaze_length %>% 
  summarise(m_gaze = mean(gaze_length_ms) / 1000)

```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (target word)" or "Look! Where's the (target word). The target words were: ball, bunny, boat, bottle, cookie, juice, chicken, and shoe. The target words varied in length (shortest = `r min(ms_words_length$m)` ms, longest = `r max(ms_words_length$m)` ms) with an average length of `r mean(ms_words_length$m) %>% round(digits=0)` ms. 

*Gaze manipulation*. To create the stimuli in the gaze condition, the speaker waited until she finished producing the target sentence and then turned her head to gaze at the bottom corner of the camera frame. After looking at the named object, she then returned her gaze to the center of the frame. We chose to allow the length of the gaze cue to vary to keep the stimuli naturalistic. The average length of gaze was `r papaja::printnum(ms_gaze_length$m_gaze)` seconds with a range from `r papaja::printnum(min(d_gaze_length$gaze_length_ms) / 1000)` to `r papaja::printnum(max(d_gaze_length$gaze_length_ms) / 1000)` seconds.

*Visual stimuli.* The image set consisted of colorful digitized pictures of objects presented in fixed pairs with no phonological overlap between the target and the distracter image (cookie-bottle, boat-juice, bunny-chicken, shoe-ball). The side of target was counterbalanced across trials.

### Procedure

Participants viewed the task on a screen while their gaze was tracked using an SMI RED corneal-reflection eye-tracker mounted on an LCD monitor, sampling at 30 Hz. The eye-tracker was first calibrated for each participant using a 6-point calibration. On each trial, participants saw two images of familiar objects on the screen for two seconds before the center stimulus appeared. Next, they processed the target sentence followed by two seconds without language to allow for a response. Both children and adults saw 32 trials (16 gaze trials; 16 no-gaze trials) with several filler trials interspersed to maintain interest. The gaze manipulation was blocked with the order of block counterbalanced across participants.

## Results and Discussion

### Timecourse looking

We first analyzed how the presence of gaze influenced listeners' distribution of attention across the three fixation locations. At target-noun onset, listeners tended to look more at the speaker than the objects. As the target noun unfolded, the mean proportion looking to the center decreased as participants shifted their gaze to the images. 
<!-- Proportion looking to the target increased sooner and reached a higher asymptote compared to proportion looking to the distracter for both gaze conditions with adults spending more time looking at the target compared to children.  -->
After looking to the named referent, listeners tended to shift their gaze back to the speaker's face. 

We did not see evidence that the presence of a post-nominal gaze cue changed how listeners allocated attention early in the target word. Children in the gaze condition, however, tended to shift their focus back to the speaker sooner after fixating on the named object, spending more time looking at the speaker throughout the trial ($p < .001$).

```{r filter for blmm}
d_model_fam <- d_fam %>% 
  filter(keep_runsheet %in% c("yes", "keep"), 
         keep_et == "include",
         rt <= 2,
         response_onset_type == "noun",
         shift_start_location == "center") %>% 
  mutate(correct_num = ifelse(shift_accuracy_clean == "correct", 1, 0))
```

```{r blmm speed-acc-fam, include = FALSE}
# RT
fit_rt_fam <- brms::brm(log(rt) ~ gaze_condition + age_group + (gaze_condition + target_image|subid), 
                        data = d_model_fam, 
                        family = gaussian(),
                        silent = TRUE,
                        file = here::here('data/04_model_outputs/speed_acc_nov_fit_rt_fam'))

params <- rownames(fixef(fit_rt_fam))

coefs_rt_fam <- fixef(fit_rt_fam) %>% 
  as_tibble() %>% 
  mutate(param = params) %>% 
  mutate_if(is.numeric, printnum) 

ms_rt_fam <- d_model_fam %>% 
  group_by(age_group, gaze_condition) %>% 
  summarise(m = mean(rt * 1000) %>% round(0))

# accuracy
fit_acc_fam <- brms::brm(correct_num ~ gaze_condition + age_group + (gaze_condition + target_image|subid), 
                         data = d_model_fam, 
                         family = binomial(link = 'logit'),
                         silent = TRUE,
                         file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_fam'))

coefs_acc_fam <- fixef(fit_acc_fam) %>% 
  as_tibble() %>% 
  mutate(param = params) %>% 
  mutate_if(is.numeric, printnum) 

ms_acc_fam <- d_model_fam %>% 
  group_by(age_group, gaze_condition) %>% 
  summarise(m = mean(correct_num) %>% round(2))
```

### First shift RT and Accuracy

Both children and adults generated similar RTs in the gaze (children $M_{rt}$ = `r ms_rt_fam$m[3]` ms, adults $M_{rt}$ = `r ms_rt_fam$m[1]` ms) and no-gaze (children $M_{rt}$ = `r ms_rt_fam$m[4]` ms, adults $M_{rt}$ = `r ms_rt_fam$m[2]` ms) conditions, with the null value falling within the 95% HDI ($\beta$ = `r coefs_rt_fam$Estimate[2]`, [`r coefs_rt_fam$Q2.5[2]`, `r  coefs_rt_fam$Q97.5[2]`]). Next, we fit the same model to estimate first shift accuracy. Adults generated more accurate gaze shifts ($M$ = `r ms_acc_fam$m[1]`) compared to children ($M$ = `r ms_acc_fam$m[3]`) with the null value falling outside the 95% HDI ($\beta_{age}$ = `r coefs_acc_fam$Estimate[3]`, [`r coefs_acc_fam$Q2.5[3]`, `r coefs_acc_fam$Q97.5[3]`]). Similar to the RT analysis, we did not find evidence of a difference in performance across the gaze conditions ($\beta$ = `r coefs_acc_fam$Estimate[2]`, [`r coefs_acc_fam$Q2.5[2]`, `r coefs_acc_fam$Q97.5[2]`]).

The time course and first shift analyses suggest that hearing a familiar noun was sufficient for both adults and children to shift visual attention away from the speaker to seek a named referent. Neither age group showed evidence of delaying eye movements to gather a social cue to reference. Children, however, did allocate more attention to the speaker after processing the familiar noun. While we did not predict these results, this behavior seems reasonable if eye movements during familiar language processing are highly-practiced visual routines such that seeking a post-nominal gaze cue becomes less-relevant for disambiguating reference. 

The results of Experiment 1 suggest that listeners do not automatically seek social information when it is available and without regard to the current language processing task; instead, it could be that they take uncertainty into account and seek additional social information when ambiguity is higher. This interpretation raises an interesting question: Would listeners adapt the timing of their gaze patterns to gather social information when they do not already know the meaning of a word? That is, when surrounded by unfamiliar objects, the value of fixating on a social partner may increase since this action could provide access to useful disambiguating information such as eye gaze -- an idea emphasized by social-pragmatic theories of language acquisition [@bloom2002children]. 

# Experiment 2

Experiment 2 explores whether learners will adapt the timing of gaze shifts to seek information from social partners when encountering a novel word. We ask three research questions: (1) do listeners adapt their gaze to seek social information in the context of processing novel words? (2) Does social information seeking change as a function of gaining more exposures to a word-object association? And (3) does following a gaze cue enhance the learning of a novel word-object link? To answer these questions, we compared the timing and accuracy of eye movements during a real-time cross-situational word learning task where participants processed sentences containing a novel word ("Where's the dax?").

We predicted that the presence of gaze would increase participants' looking to a speaker, leading to a higher proportion of fixations to the social target and slower first shift reaction times to the objects. This prediction translates to a main effect of gaze condition on proportion looking to the speaker and on first shift RTs. We also predicted a trial number by gaze condition interaction such that the decrease in RT will be larger on exposure trials in the gaze condition, reflecting a reduction in the need to seek social information after learning the word-object mappings. Finally, we predicted that the presence of gaze would lead to faster learning of the novel word-object links, which would result in more accurate first shifts, faster RTs, and a higher proportion looking to the target object on test trials in the gaze condition.

```{r san-prop-looking-plot, fig.env = "figure*", fig.pos = "t", fig.width=4, fig.asp = 0.4, out.width= "90%", fig.cap = "Panel A shows participantsâ€™ tendency to look at the speaker on exposure and test trials as a function of the trial number within a learning block. The horizontal, dashed line represents the tendency to distribute attention equally across the three AOIs. Color indicates gaze condition and error bars represent 95\\% credible intervals. Panel B shows the same information but for target and distracter looking across the learning block."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_proplook.jpeg"))
```

## Methods

### Participants

```{r filter timecourse data}
d_analysis_nov <- d_novel_tc %>% 
  filter(keep_drop == "keep",
         t_rel_noun >= 0, 
         t_rel_noun <= 3)

d_analysis_nov %<>% 
  split(.$subid) %>%  
  purrr::map_dfr(create_time_bins_ss, t_ms_diff = 33)

# exclude based on runsheet information
d_exclusions_rs <- d_novel_tc %>% 
  distinct(subid, keep_drop, reason_excluded, age_category) %>% 
  filter(keep_drop == "drop") 

# exclude based on number of trials completed.
d_exclusions_et <- d_analysis_nov %>%
  filter(target_looking != "away") %>% 
  distinct(subid, trial_num_exp, age_category) %>% 
  dplyr::count(subid, age_category) %>% 
  filter(n < 16) %>% 
  mutate(keep_drop = "drop", 
         reason_excluded = "fewer than half trials completed") 

# create subid vector for filtering later
ss_exclude <- d_exclusions_et %>% pull(subid)

# Join exclusions tables and save to disk.
d_exclusions_final <- d_exclusions_et %>% 
  dplyr::select(-n) %>% 
  bind_rows(d_exclusions_rs)

d_analysis_nov %<>% filter(!(subid %in% ss_exclude))

## flag trials with less than half looking to AOIs
trial_filter <- d_analysis_nov %>% 
  distinct(subid, trial_num_exp, learning_block, gaze_condition, target_looking, time_ms_normalized) %>% 
  count(subid, trial_num_exp, learning_block, gaze_condition, target_looking) %>% 
  group_by(trial_num_exp, subid) %>% 
  mutate(total_samples = sum(n),
         prop_trial = n / total_samples,
         good_trial = case_when(
           target_looking == "away" & prop_trial >= 0.5 ~ "drop",
           TRUE ~ "keep"
         )) %>% 
  distinct(subid, trial_num_exp, good_trial)

d_analysis_nov %<>% left_join(trial_filter)

n_samples_trial <- d_analysis_nov %>% 
  count(subid, trial_num_exp) %>% 
  rename(n_samples_trial = n)

d_analysis_nov %<>% left_join(n_samples_trial)

d_analysis_nov %<>% 
  filter(target_looking != "away", good_trial == "keep") %>% 
  mutate(age_category = ifelse(age_category == "child", "children", "adults")) %>% 
  mutate(age_category = factor(age_category, levels = c("children", "adults")))
```

```{r exclusions-e3}
d_excluded_e3 <- d_exclusions_final %>% count(reason_excluded, age_category)
```

Participants were native, monolingual English-learning children ($n=$ `r kids_age_table$n[2]`; `r gender_breakdown$n[3]` F) and adults ($n=$ `r n_adults_total$n[2]`; `r n_adults_gender$n[1]` F). All participants had no reported history of developmental or language delay and normal vision. `r d_excluded_e3$n[2]` adults were run but not included because they were not native speakers of English. `r d_excluded_e3$n[1]` children participants were run but not included in the analysis because the participant did not complete more than half of the trials in the task.

### Materials

```{r speed-acc-nov-stim}
d_nov_stim <- read_csv(here::here("data/00_stimuli_information/analysis_order_sheets/speed-acc-novel-analysis-order-sheet.csv"))

# compute length of words
nov_word_stim <- d_nov_stim %>% 
  select(stimulus_name, noun_onset_sec, noun_onset_frames, noun_offset_sec, noun_offset_frames) %>% 
  unique() %>% 
  mutate(noun_onset_ms = (noun_onset_sec * 1000) + (noun_onset_frames * 33),
         noun_offset_ms = (noun_offset_sec * 1000) + (noun_offset_frames * 33),
         noun_length_ms = (noun_offset_ms - noun_onset_ms ) %>% round(0)
  ) %>% 
  summarise(m = mean(noun_length_ms), 
            min_word = min(noun_length_ms),
            max_word = max(noun_length_ms))

# compute length of gaze
nov_gaze_stim <- d_nov_stim %>% 
  select(stimulus_name, gaze_onset_sec, gaze_onset_frames, gaze_offset_sec, gaze_offset_frames) %>% 
  unique() %>% 
  filter(!is.na(gaze_onset_sec)) %>% 
  mutate(gaze_onset_ms = (gaze_onset_sec * 1000) + (gaze_onset_frames * 33),
         gaze_offset_ms = (gaze_offset_sec * 1000) + (gaze_offset_frames * 33),
         gaze_length_ms = gaze_offset_ms - gaze_onset_ms
  ) %>% 
  summarise(m = mean(gaze_length_ms), 
            min_gaze = min(gaze_length_ms),
            max_gaze = max(gaze_length_ms))
```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (novel word)" or "Look! Where's the (novel word). The target words were four pseudo-words: bosa, modi, toma, and pifo. The novel words varied in length (shortest = `r papaja::printnum(nov_word_stim$min_word)` ms, longest = `r papaja::printnum(nov_word_stim$max_word)` ms) with an average length of `r papaja::printnum(nov_word_stim$m)` ms. 

*Gaze manipulation*. The gaze manipulation was identical to Experiment 1. The average length of gaze was `r papaja::printnum(nov_gaze_stim$m / 1000)` seconds with a range from `r papaja::printnum(nov_gaze_stim$min_gaze / 1000)` to `r papaja::printnum(nov_gaze_stim$max_gaze / 1000)` seconds.

*Visual stimuli.* The image set consisted of 28 colorful digitized pictures of objects that were selected such that they would be interesting to and that children would be unlikely to have already a label associated with the objects. The side of the target picture was counterbalanced across trials.

### Procedure

The procedure was identical to Experiment 1. Participants watched a series of ambiguous word learning events organized into pairs of one exposure and one test trial. On each trial, participants saw a set of two unfamiliar objects and heard one novel word. Each word occurred in a block of four exposure-test pairs for a total of eight trials for each novel word. Exposure trials in the gaze condition included a gaze cue. Test trials in both conditions did not include a gaze.cue.On each trial within a learning block, one of the objects in the set had consistently appeared on the previous trials (target object), while the other object was a randomly generated novel object that had not been shown in the experiment (distracter object). 
<!-- Both children and adults saw 32 trials (16 gaze trials; 16 no-gaze trials) with several filler trials interspersed to maintain interest. The gaze manipulation was presented in a blocked design with the order of block counterbalanced across participants. -->

## Results and Discussion

<!-- ### Timecourse looking -->

<!-- ```{r san-tc-plot, include = F, fig.env = "figure*", fig.pos = "t", fig.width=5, fig.asp = 0.6, out.width= "85%", fig.cap = "Overview of children and adults' looking to the three fixation targets (Speaker, Target, Distracter) over the course of exposure and test trials. Panel A shows proportion looking to the speaker's face with color indicating gaze condition and line type indicating age group. Panel B shows the same information but for proportion looking to the target and distracter images."} -->

<!-- knitr::include_graphics(here::here(image_path, "speed_acc_novel_tc.jpeg")) -->
<!-- ``` -->

<!-- *Looking to the speaker.* How did the presence of a gaze cue change learners' decisions to fixate on the speaker? Visual inspection of Figure \ref{fig:san-tc-plot}A shows that both children and adults tended to start looking at the speaker at noun onset and shifted their gaze away as the noun unfolded, with adults doing so sooner compared to children. On exposure trials when there was a gaze cue, both adults and children tended to look more to the face at noun onset as indicated by the higher intercept of the blue curves. Moreover, around one second after noun onset, listeners tended to shift their attention back to the speaker's face more often and especially so for children. This pattern of looking parallels the effect of gaze on children's time course of fixations while processing familiar words in Experiment 1. On test trials that were preceded by an exposure trial with gaze, children and adults tended to look more to the speaker even though there was no gaze cue present. This pattern suggests that the presence of gaze likely modulated learners' expectations of gathering disambiguating information from the speaker on test trials. -->

<!-- *Looking to the target and distracter.* Next, we asked how learners divided attention between the target and distracter objects. On exposure trials, looking to both objects increased throughout the trial but more so for looks to the named object as indicated by the higher asymptote of the target looking curves. Adults spent more time looking to the target and less time looking to the distracter as compared to children. Interestingly, when there was a gaze cue to process, children and adults allocated fewer fixations to the distracter object, providing evidence that social information could reduce the potential for learners to create spurious word-object links during statistical word learning.  -->

```{r aggregate prop looking}
ss_groupings <- list("subid", "gaze_condition", "trial_type", 
                     "age_category", "trial_num_learn_block", "trial_num_exp")
ms_groupings <- list("subid", "gaze_condition", "trial_type", 
                     "age_category", "trial_num_learn_block")

ss_novel_face <- d_analysis_nov %>% 
  aggregate_ss_looking(df = ., 
                       ss_groupings, 
                       ms_groupings, 
                       aoi_column = "target_looking", 
                       return_ss_df = TRUE) 

ss_novel_obj <- d_analysis_nov %>% 
  filter(target_looking != "center") %>% 
  aggregate_ss_looking(df = ., 
                       ss_groupings, 
                       ms_groupings, 
                       aoi_column = "target_looking", 
                       return_ss_df = TRUE) 
```

```{r fit prop looking speaker blmm, include = FALSE}
fit_proplook_face_nov <- ss_novel_face %>% 
  filter(target_looking == "center", trial_type == "exposure") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + age_category + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block | subid),
            family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_face')
  )

params_proplook_face <- rownames(fixef(fit_proplook_face_nov))

coefs_proplook_face_nov <- fixef(fit_proplook_face_nov) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_face) %>% 
  mutate_if(is.numeric, round, 2) 
```

```{r fit target looking compared to chance, include = FALSE}
fit_chance <- ss_novel_obj %>% 
  filter(target_looking == "target", trial_type == "test") %>% 
  brms::brm(data = ., 
            prop_looking ~ gaze_condition + age_category + (gaze_condition | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_chance')
  )

coefs_chance_nov <- fit_chance %>% 
  spread_draws(b_gaze_conditionstraight_ahead, b_Intercept, b_age_categoryadults) %>%
  mutate(no_gaze_adults = b_Intercept + b_gaze_conditionstraight_ahead + b_age_categoryadults, 
         gaze_adults = b_Intercept + b_age_categoryadults,
         no_gaze_kids = b_Intercept + b_gaze_conditionstraight_ahead) %>%
  rename(gaze_kids = b_Intercept) %>% 
  select(.draw, gaze_kids, no_gaze_adults:no_gaze_kids) %>% 
  gather(param, value, -.draw) %>% 
  group_by(param) %>% 
  summarise(m = quantile(value, probs = 0.5),
            ci_lower = quantile(value, probs = 0.05),
            ci_upper = quantile(value, probs = 0.95))
```

```{r fit prop looking target blmm, include = FALSE}
fit_proplook_target_nov_all <- ss_novel_obj %>% 
  filter(target_looking == "target") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + trial_type + age_category + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block + trial_type | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_targ_all')
  )

params_proplook_targ <- rownames(fixef(fit_proplook_target_nov_all))

coefs_proplook_targ_nov <- fixef(fit_proplook_target_nov_all) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_targ) %>% 
  mutate_if(is.numeric, round, 2) 

## fit separate models for the adults and kids
## this is totally exploratory and just fit to answer 
## the question of whether we could measure learning in kids, would we
## expect to see an effect of the gaze manipulation on learning trajectories

fit_proplook_target_nov_adults <- ss_novel_obj %>% 
  filter(target_looking == "target", age_category == "adults") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + trial_type + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block + trial_type | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_targ_adults')
  )

params_proplook_targ_adults <- rownames(fixef(fit_proplook_target_nov_adults))

coefs_proplook_targ_nov_adults <- fixef(fit_proplook_target_nov_adults) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_targ_adults) %>% 
  mutate_if(is.numeric, round, 2) 

## Kids model
fit_proplook_target_nov_kids <- ss_novel_obj %>% 
  filter(target_looking == "target", age_category == "children") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + trial_type + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block + trial_type | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_targ_kids')
  )

params_proplook_targ_kids <- rownames(fixef(fit_proplook_target_nov_kids))

coefs_proplook_targ_nov_kids <- fixef(fit_proplook_target_nov_kids) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_targ_kids) %>% 
  mutate_if(is.numeric, round, 2) 

## get summary stats for prop looking to target
ss_novel_obj %>% 
  filter(target_looking == "target") %>% 
  group_by(age_category, trial_type, gaze_condition) %>% 
  summarise(m = mean(prop_looking))
```

```{r speed-acc-novel exposure-test looking aggregate}
ss_groupings <- list("subid", "gaze_condition", "trial_type", "age_category", "trial_num_learn_block", "target_word",  "trial_num_exp")
ms_groupings <- list("subid", "gaze_condition", "trial_type", "age_category", "trial_num_learn_block", "target_word")

ss_novel_learning <- d_analysis_nov %>% 
  #filter(target_looking != "center") %>% 
  aggregate_ss_looking(df = .,
                       ss_groupings, 
                       ms_groupings, 
                       aoi_column = "target_looking", 
                       return_ss_df = TRUE) 

exposure_ss <- ss_novel_learning %>% 
  filter(target_looking == "target") %>% 
  select(subid, trial_num_exp, trial_num_learn_block, trial_num_exp, 
         target_word, age_category, gaze_condition, trial_type, prop_looking) %>% 
  spread(trial_type, prop_looking) %>% 
  filter(!is.na(exposure)) %>% 
  select(-test, -trial_num_exp)

test_ss <- ss_novel_learning %>% 
  filter(target_looking == "target") %>% 
  select(subid, trial_num_exp, trial_num_learn_block, trial_num_exp, 
         target_word, age_category, gaze_condition, trial_type, prop_looking) %>%  
  spread(trial_type, prop_looking) %>% 
  filter(!is.na(test)) %>% 
  select(-exposure, -trial_num_exp)

ss_learning_final <- left_join(exposure_ss, test_ss)
```

```{r blmm speed-acc-novel exp-test looking, include=FALSE}
fit_exp_test_look <- ss_learning_final %>% 
  brms::brm(data = ., 
            test ~ (exposure + gaze_condition + age_category + trial_num_learn_block)^2 + (gaze_condition + trial_num_learn_block | subid),
            family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_exp_test')
  )

params_exp_test <- rownames(fixef(fit_exp_test_look))

coefs_exp_test <- fixef(fit_exp_test_look) %>% 
  as_tibble() %>% 
  mutate(param = params_exp_test) %>% 
  mutate_if(is.numeric, printnum) 
```

```{r speed-acc-nov filter for blmm}
d_model_nov <- d_novel_fs %>%
  filter(is.na(block_excluded) | block_excluded != learning_block, 
         shift_start_location == "center") %>%
  group_by(subid, learning_block) %>%
  mutate(n_trials = n()) %>% 
  filter(rt > 0, 
         keep_drop == "keep", 
         n_trials >= 2) %>% 
  ungroup()

m_rt <- log(d_model_nov$rt) %>% mean()
sd_rt <- log(d_model_nov$rt) %>% sd()

d_model_nov %<>% 
  mutate(log_rt = log(rt),
         keep_rt = case_when(
           log_rt <= m_rt + 3*sd_rt & log_rt >= m_rt - 3*sd_rt ~ "keep",
           TRUE ~ "drop"
         )) %>% 
  filter(keep_rt == "keep")
```

```{r blmm rt speed-acc-novel, include = FALSE}
# RT
fit_rt_nov <- brms::brm(log(rt) ~ (gaze_on_trial + age_category + learn_block_half)^2 + 
                          (gaze_on_trial|subid), 
                        data = filter(d_model_nov, trial_type == "exposure"),
                        family = gaussian(),
                        silent = TRUE,
                        file = here::here('data/04_model_outputs/speed_acc_nov_fit_rt_nov_exp'))

params <- rownames(fixef(fit_rt_nov))

coefs_rt_nov <- fixef(fit_rt_nov) %>% 
  as_tibble() %>% 
  mutate(param = params) %>% 
  mutate_if(is.numeric, printnum) 

## summary stats for RT
ms_rt_nov <- d_model_nov %>% 
  filter(keep_rt == "keep", trial_type == "exposure") %>% 
  group_by(age_category, gaze_on_trial, learn_block_half) %>% 
  summarise(m = mean(rt) * 1000)

fit_rt_nov_test <- brms::brm(log(rt) ~ (gaze_on_trial + age_category + learn_block_half)^2 + 
                               (gaze_on_trial|subid), 
                             data = filter(d_model_nov, trial_type == "exposure"),
                             family = gaussian(),
                             silent = TRUE,
                             file = here::here('data/04_model_outputs/speed_acc_nov_fit_rt_nov_test'))

params_rt_test <- rownames(fixef(fit_rt_nov_test))

coefs_rt_nov_test <- fixef(fit_rt_nov_test) %>% 
  as_tibble() %>% 
  mutate(param = params_rt_test) %>% 
  mutate_if(is.numeric, printnum) 
```

```{r blmm accuracy speed-acc-novel, include=FALSE}
# accuracy exposure trials
fit_acc_nov_exp <- brms::brm(correct_num ~ (gaze_on_trial + age_category + learn_block_half)^2 + (gaze_on_trial|subid),
                             data = filter(d_model_nov, trial_type == "exposure"), 
                             family = binomial(link = 'logit'),
                             silent = TRUE,
                             file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_exp'))

params_acc <- rownames(fixef(fit_acc_nov_exp))

coefs_acc_nov_exp <- fixef(fit_acc_nov_exp) %>% 
  as_tibble() %>% 
  mutate(param = params_acc) %>% 
  mutate_if(is.numeric, printnum)

# d_model_nov %>% 
#   filter(keep_rt == "keep", trial_type == "exposure") %>% 
#   group_by(age_category, gaze_on_trial, learn_block_half) %>% 
#   summarise(m = mean(correct_num)) %>% 
#   ggplot(aes(x = learn_block_half, y = m, color = gaze_on_trial)) +
#   geom_point() + 
#   geom_line(aes(group=gaze_on_trial)) +
#   facet_wrap(~age_category)

# accuracy test trials
fit_acc_nov_test <- brms::brm(correct_num ~ (gaze_condition + age_category + trial_num_learn_block)^2 + 
                                (gaze_condition + trial_num_learn_block|subid),
                              data = filter(d_model_nov, trial_type == "test"), 
                              family = binomial(link = 'logit'),
                              silent = TRUE,
                              file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_test'))

params_acc_test <- rownames(fixef(fit_acc_nov_test))

coefs_acc_nov_test <- fixef(fit_acc_nov_test) %>% 
  as_tibble() %>% 
  mutate(param = params_acc_test) %>% 
  mutate_if(is.numeric, printnum)  

# accuracy test trials
fit_acc_nov_test_no_int <- brms::brm(correct_num ~ (gaze_condition + age_category + trial_num_learn_block) + 
                                       (gaze_condition + trial_num_learn_block|subid),
                                     data = filter(d_model_nov, trial_type == "test"), 
                                     family = binomial(link = 'logit'),
                                     silent = TRUE,
                                     file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_test_noint'))

params_acc_test_noint <- rownames(fixef(fit_acc_nov_test_no_int))

coefs_acc_nov_test_noint <- fixef(fit_acc_nov_test_no_int) %>% 
  as_tibble() %>% 
  mutate(param = params_acc_test_noint) %>% 
  mutate_if(is.numeric, printnum)

# d_model_nov %>% 
#   filter(keep_rt == "keep", trial_type == "test") %>% 
#   group_by(age_category, gaze_condition, trial_num_learn_block) %>% 
#   summarise(m = mean(correct_num)) %>% 
#   ggplot(aes(x = trial_num_learn_block, y = m, color = gaze_condition)) +
#   geom_point() + 
#   geom_line(aes(group=gaze_condition)) +
#   facet_wrap(~age_category)
```

```{r speed-acc-novel summary-stats}
## summary stats for RT
ms_rt_nov <- d_model_nov %>% 
  filter(keep_rt == "keep") %>% 
  group_by(age_category, gaze_on_trial) %>% 
  summarise(m = mean(rt) * 1000) %>% 
  mutate(m = papaja::printnum(m))

## summary stats for accuracy 
ms_acc_nov <- d_model_nov %>% 
  group_by(age_category, gaze_condition, trial_type) %>% 
  summarise(m = mean(correct_num) %>% papaja::printnum())
```

### Prediction 1: The presence of gaze increases social information seeking.

Our primary question of interest was whether listeners would seek a post-nominal gaze cue when processing novel words. In line with our prediction, we found evidence that both children and adults spent more time fixating on a speaker when she provided a social cue to reference (Fig. 3A $\beta_{gaze}$ = `r papaja::printnum(coefs_proplook_face_nov$Estimate[2] * -1)` [`r papaja::printnum(coefs_proplook_face_nov$Q2.5[2] * -1)`, `r papaja::printnum(coefs_proplook_face_nov$Q97.5[2] * -1)`]). Moreover, both children (Gaze $M_{rt}$ = `r ms_rt_nov$m[1]` ms, No-gaze $M_{rt}$ = `r ms_rt_nov$m[2]` ms)  and adults (Gaze $M_{rt}$ = `r ms_rt_nov$m[3]` ms, No-gaze $M_{rt}$ = `r ms_rt_nov$m[4]` ms) had slower RTs in the gaze condition ($\beta_{gaze}$ = `r coefs_rt_nov$Estimate[2]`, [`r coefs_rt_nov$Q2.5[2]`, `r coefs_rt_nov$Q97.5[2]`]), with no evidence of an interaction between gaze condition and age group  ($\beta_{age*gaze}$ = `r coefs_rt_nov$Estimate[4]`, [`r coefs_rt_nov$Q2.5[4]`, `r coefs_rt_nov$Q97.5[4]`]). This result provides support for our key prediction that listeners would adapt their gaze to seek social information when uncertainty over the meaning of words was higher.

### Prediction 2: The tendency to fixate on a speaker decreases faster when learning from gaze.

We predicted that RTs on exposure trials in the gaze condition would decrease at a faster rate, reflecting a reduction in the need to seek social information after being exposed to consistent word-object pairings. In contrast to our prediction, we found a developmental difference such that children, but not adults, were more likely to *increase* their tendency to fixate on the speaker throughout the learning block (Fig 3A, $\beta_{age*tr.num}$ = `r papaja::printnum(coefs_proplook_face_nov$Estimate[7])`, [`r papaja::printnum(coefs_proplook_face_nov$Q2.5[7])`, `r papaja::printnum(coefs_proplook_face_nov$Q97.5[7])`]). This developmental difference suggests that looking to a social partner may have been more useful for children who were still trying to disambiguate the novel words; whereas adults showed evidence of successful disambiguation after the second exposure trial and could focus attention on the objects instead.

### Prediction 3: Faster learning of novel words from gaze cues.

Both children and adults showed evidence of learning the novel word-object links by the end of the task, with the null value of 0.5 falling below the lower bound of the lowest credible interval for children's target looking in the No-gaze context (95% HDI [`r papaja::printnum(coefs_chance_nov$ci_lower[4])`, `r papaja::printnum(coefs_chance_nov$ci_upper[4])`]). Looking to the target increased as listeners were exposed to more word-object pairings during the task ($\beta_{tr.num}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[4])`, [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[4])`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[4])`]) and was higher when the novel word was accompanied by a gaze cue ($\beta_{gaze}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[2] * -1)`, [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[2] * -1)`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[2] * -1)`]). 

Visual inspection of the top row of Fig \ref{fig:san-prop-looking-plot}B shows that on the first exposure trial, both adults and children used the gaze cue to disambiguate reference, fixating more on the target in the gaze condition. In contrast, adults target looking reached ceiling for both the gaze and no-gaze conditions by trial number two, indicating that they had successfully used co-occurrence information to learn the words. Finally, on test trials, adults tended to look more to the target object when learning from a gaze cue, only reaching similar levels of accuracy in the no-gaze condition at the end of the learning block. There was not substantial evidence, however, that the gaze manipulation influenced children's looking behavior on test trials when there was no gaze cue present, with children showing comparable and relatively low-levels of accuracy overall.

Contrary to our prediction, we did not see evidence that the gaze manipulation led children or adults to generate more accurate first shifts on test trials ($\beta_{gaze}$ = `r coefs_acc_nov_test_noint$Estimate[2]`, [`r coefs_acc_nov_test_noint$Q2.5[2]`, `r  coefs_acc_nov_test_noint$Q97.5[2]`]) or a faster increase in first shift accuracy over the course of learning ($\beta_{gaze*tr.num}$ = `r coefs_acc_nov_test$Estimate[6]`, [`r coefs_acc_nov_test$Q2.5[6]`, `r  coefs_acc_nov_test$Q97.5[6]`]), with the null value falling within each credible interval.


<!-- ### Proportion looking -->

<!-- *Learning effects.* Both children ($M_{gaze}$ = `r papaja::printnum(coefs_chance_nov$m[2])`, $M_{no-gaze}$ = `r papaja::printnum(coefs_chance_nov$m[4])`) and adults ($M_{gaze}$ = `r papaja::printnum(coefs_chance_nov$m[1])`, $M_{no-gaze}$ = `r papaja::printnum(coefs_chance_nov$m[3])`) showed evidence of learning the novel word-object links, with the null value of 0.5 falling below the lower bound of the lowest credible interval for children's target looking in the No-gaze context (95% HDI [`r papaja::printnum(coefs_chance_nov$ci_lower[4])`, `r papaja::printnum(coefs_chance_nov$ci_upper[4])`]). Our primary question of interest was how exposure to multiple co-occurrences of word-object pairs would change learners' distribution of attention between the speaker and objects. Both children and adults were more likely to fixate on the speaker when she provided a gaze cue ($\beta_{gaze}$ = `r papaja::printnum(coefs_proplook_face_nov$Estimate[2] * -1)` [`r papaja::printnum(coefs_proplook_face_nov$Q2.5[2] * -1)`, `r papaja::printnum(coefs_proplook_face_nov$Q97.5[2] * -1)`]). Moreover, there was a developmental difference such that children, but not adults, were more likely to increase their fixations to the speaker over the course of the learning block (Fig 4A, $\beta_{age*tr.num}$ = `r papaja::printnum(coefs_proplook_face_nov$Estimate[7])`, [`r papaja::printnum(coefs_proplook_face_nov$Q2.5[7])`, `r papaja::printnum(coefs_proplook_face_nov$Q97.5[7])`]).  -->

<!-- Overall, looking to the target increased as learners were exposed to more word-object pairings ($\beta_{tr.num}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[4])`, [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[4])`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[4])`]) and was higher when the novel word was accompanied by a gaze cue ($\beta_{gaze}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[2] * -1)`, [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[2] * -1)`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[2] * -1)`]). Visual inspection of the top row of Fig \ref{fig:san-prop-looking-plot}B shows that on the first exposure trial, both adults and children used the gaze cue to disambiguate reference, fixating more on the target in the gaze condition. For children, higher target looking on exposure trials with gaze remained relatively constant across the learning block. In contrast, adults target looking reached ceiling for both the gaze and no-gaze conditions by trial number two, indicating that they had successfully used co-occurrence information to learn the words. For adults, we found an interaction between gaze condition and trial number such that looking to the target increased more quickly in the No-gaze condition ($\beta_{gaze*tr.num}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[8])`, [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[8])`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[8])`]), which reflects (1) the higher intercept of target looking in the presence of gaze and (2) rapid learning of the word-object association via cross-situational information (bottom row of Fig 4B). Finally, visual inspection of test trials in Fig 4B suggests that adults tended to look more the target when learning from a gaze cue, only reaching similar levels of accuracy in the no-gaze condition at the end of the learning block. There was not strong evidence that the gaze manipulation influenced children's looking behavior on test trials.  -->

<!-- *Relationship between looking on exposure and test.* For both children and adults, more time attending to the target object on exposure trials led to a higher proportion of looking to the target on test trials, especially for adults ($\beta_{exposure*age}$ = `r papaja::printnum(coefs_exp_test$Estimate[7])`, [`r papaja::printnum(coefs_exp_test$Q2.5[7])`, `r papaja::printnum(coefs_exp_test$Q97.5[7])`]) and as the number of word-object exposures increased over the course a learning block ($\beta_{exposure*tr.num}$ = `r papaja::printnum(coefs_exp_test$Estimate[8])`, [`r papaja::printnum(coefs_exp_test$Q2.5[8])`, `r papaja::printnum(coefs_exp_test$Q97.5[8])`]). There was evidence that participants in the No-gaze condition showed less learning over the course of the word block ($\beta_{gaze*tr.num}$ = `r papaja::printnum(coefs_exp_test$Estimate[10])`, [`r papaja::printnum(coefs_exp_test$Q2.5[10])`, `r papaja::printnum(coefs_exp_test$Q97.5[10])`]). This result provides evidence that the presence of social information did more than change attention on exposure trials but instead modulated the relationship between attention during learning and later memory for word-object links. -->

<!-- In sum, these reults suggest that the presence of gaze changed how children and adults allocated attention while processing novel words. In the context of unfamiliar objects, children tended to fixate more on a speaker's face when she provided a post-nominal social cue to reference, a difference in looking behavior that increased as they were exposed to more word-object co-occurrences. This result is different from Experiment 1 where listeners showed similar gaze dynamics when processing familiar nouns. Moreover, when a gaze cue was available, children and adults spent less time fixating on the distracter object, which could modulate the associations that learners store from labeling event. The effect of gaze, however, did not generalize to performance on test trials for children. -->

<!-- ### First shift RT and Accuracy -->

```{r speed-acc-novel-shifts, include=F, out.width="90%", fig.cap = "First shift Reaction Time (RT), and Accuracy results for children and adults in Experiment 2. Panel A shows the distribution of pairwise contrasts between RTs in the gaze and no-gaze conditions. The square point represents the mean value for each measure. The vertical dashed line represents the null model of zero condition difference. The width each point represents the 95\\% HDI. Panel B shows the same information but for participants' first shift accuracy."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_fstshifts.jpeg"))
```

<!-- We next asked how the presence of gaze influenced learners' decision to stop gathering visual information from the speaker and start fixating on the novel objects. Both children (Gaze $M_{rt}$ = `r ms_rt_nov$m[1]` ms, No-gaze $M_{rt}$ = `r ms_rt_nov$m[2]` ms)  and adults (Gaze $M_{rt}$ = `r ms_rt_nov$m[3]` ms, No-gaze $M_{rt}$ = `r ms_rt_nov$m[4]` ms) fixated longer on the speaker when she provided a gaze cue ($\beta_{gaze}$ = `r coefs_rt_nov$Estimate[2]`, [`r coefs_rt_nov$Q2.5[2]`, `r coefs_rt_nov$Q97.5[2]`]). With no evidence of an interaction between gaze condition and age group  ($\beta_{age*gaze}$ = `r coefs_rt_nov$Estimate[4]`, [`r coefs_rt_nov$Q2.5[4]`, `r coefs_rt_nov$Q97.5[4]`]). Moreover, both children (Gaze $M_{acc}$ = `r ms_acc_nov$m[1]`, No-gaze $M_{acc}$ = `r ms_acc_nov$m[3]`)  and adults (Gaze $M_{acc}$ = `r ms_acc_nov$m[5]`, No-gaze $M_{acc}$ = `r ms_acc_nov$m[7]`) generated more accurate first shifts in the gaze condition, indicating they were following the gaze cue on exposure trials ($\beta_{gaze}$ = `r coefs_acc_nov_exp$Estimate[2]`, [`r coefs_acc_nov_exp$Q2.5[2]`, `r  coefs_acc_nov_exp$Q97.5[2]`]).  -->

<!-- Finally, we asked whether the presence of gaze affected learning by predicting first shift accuracy on test trials. We found that adults were more accurate than children ($\beta_{age}$ = `r coefs_acc_nov_test_noint$Estimate[3]`, [`r coefs_acc_nov_test_noint$Q2.5[3]`, `r  coefs_acc_nov_test_noint$Q97.5[3]`]), that first shifts became more accurate as learners experienced repeated exposures to word-object pairings ($\beta_{tr.num}$ = `r coefs_acc_nov_test_noint$Estimate[4]`, [`r coefs_acc_nov_test_noint$Q2.5[4]`, `r  coefs_acc_nov_test_noint$Q97.5[4]`]). We did not see evidence for two of our predictions: (1) that children and adults would generate more accurate first shifts when learning from social gaze ($\beta_{gaze}$ = `r coefs_acc_nov_test_noint$Estimate[2]`, [`r coefs_acc_nov_test_noint$Q2.5[2]`, `r  coefs_acc_nov_test_noint$Q97.5[2]`]) and (2) that learning from gaze would modulate the relationship between accuracy over the course of learning ($\beta_{gaze*tr.num}$ = `r coefs_acc_nov_test$Estimate[6]`, [`r coefs_acc_nov_test$Q2.5[6]`, `r  coefs_acc_nov_test$Q97.5[6]`]), with the null value falling within each credible interval.  -->

<!-- Returning to our three behavioral predictions, we found evidence that both children and adults spent more time fixating on a speaker when she provided a useful social cue to reference. Moreover, adults decreased the amount of time fixating on the speaker as they gained more exposures to the word-object pairings, but children showed the opposite pattern, increasing their fixations to the speaker later in the task. This developmental difference suggests that looking to a social partner may have been more useful for children who were still trying to disambiguate the novel words; whereas adults showed evidence of successful disambiguation after the second exposure trial and could focus attention on the objects instead. Finally, we found mixed evidence that the presence of gaze modulated the relationship between visual attention during labeling and learning of the novel word-object mappings. Both children and adults generated a higher proportion of shifts landing on the target when there was post-nominal gaze cue available. But only adults spent more time fixating on the target object and generated more accurate first shifts for words learned with a gaze cue. -->

# General Discussion

Does social information seeking change as a function of a speaker's helpfulness and listeners' current task goals? Here, we pursued the idea that listeners flexibly adapt their eye movements to gather social gaze when it was useful. We found that children and adults did not automatically delay their gaze shifts to seek a post-nominal gaze cue while processing familiar words. However, when processing novel words, both children and adults fixated more on a speaker to seek a post-nominal gaze cue. This delay resulted in more attention allocated to the named object and less looking to the distracter object, an effect that increased throughout the task for children. Moreover, adults, but not children, showed evidence of stronger learning in the presence of social gaze while both age groups were capable of learning the word-object pairings from cross-situational statistics alone.

How should we characterize the effects of gaze in our task? Children selectively gathered social information when they were uncertain about the meaning of a new word, focusing more attention on a single object. Moreover, the presence of gaze increased children's tendency to fixate on the speaker, an increase in attention that generalized to trials without a gaze cue. This pattern of behavior shows how the presence of social information can accumulate over time, modulating the visual information that children gather during language processing. Finally, seeking social gaze increased the rate of word learning for adults. This finding dovetails with work showing how the presence of social cues can change information processing (e.g., the type of features remembered from an event) [@yoon2008communication]. 

This work has several important limitations. First, we did not find strong evidence that the effects of learning novel words from gaze generalized to contexts without gaze for children in Experiment 2. Moreover, children did not show robust learning of the novel word-object links overall. Prior work has shown that 3-5 year-olds learn words better from an extended, as opposed to brief, social cue to reference [@yurovsky2013online]. Future work could increase the length of the gaze cue, which was relatively short in these studies (~2 sec) or could include a more extensive set of cues to reference such as pointing or holding objects. 

Second, our experimental task measures changes in gaze dynamics in a simplified visual and linguistic context. While this approach has the benefit of providing a high degree of control, it limits our ability to generalize these results to language processing in more natural contexts. That is, in conversation, social partners can actively monitor listeners for comprehension and produce contingent responses to ensure understanding. This gap suggests two useful next steps: (1) measure changes in social information seeking within naturalistic conversations, and (2) develop lab-based methods that include contingent responding to children's actions.

These results show that even young listeners are sensitive to the informational tradeoffs in active information gathering. We found that both children and adults' decisions to seek social information varied depending on their uncertainty over word-object mappings. In the context of processing novel, but not familiar words, listeners adapted their gaze to seek a post-nominal social cue to reference. This change led to increased visual attention on a single object and less attention distributed across potential spurious word-object links. This approach sheds light on how children might integrate their prior knowledge accumulated via statistical information when deciding whether to seek information from their interlocutors during real-time language comprehension and learning.

<!-- \newpage -->

```{r session info, include = F}
sessionInfo() %>% pander::pander(compact = F)
```

\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering Data/code available at \url{https://bit.ly/2FgIbsW} \\ E1 preregistration at \url{https://osf.io/2q4gw/}\\ E2 preregistration at \url{https://osf.io/nfz85/}}} \vspace{1em}

# Acknowledgements

Thanks to K. Constandse, T. Alade, and H. Slater for help with data collection. This work was supported by an NSF GRFP to KM and a Jacobs Foundation Fellowship to MCF.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
