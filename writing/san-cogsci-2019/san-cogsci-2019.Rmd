---
title: "Children integrate social and statistical information during language processing"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Kyle MacDonald (kemacdonald@ucla.edu)} \\ Department of Communication, UCLA 
    \AND {\large \bf Elizabeth Swanson (elizswan@stanford.edu)} \\ Department of Psychology, Stanford University 
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, Stanford University 
    }

abstract: >
    How do children learn words from input that has many possible word-object mappings? Statistical learning allows children to aggregate consistent word-object co-occurrences to reduce uncertainty over time, while social-pragmatic cues can constrain ambiguity within a labeling event. Here, we present two eye-tracking studies asking how children integrate statistical and social information during real-time language processing. When processing familiar words, children and adults did not delay their gaze shifts to seek a post-nominal social cue to reference (eye gaze). When processing novel words, however, children and adults fixated longer on a speaker who produced a gaze cue, which, in turn, led to an increase in looking to a named object and less looking to the other objects in the scene. These results suggest that learners integrate their knowledge of object labels when deciding how to allocate visual attention to social partners, which in turn can shape the input to word learning mechanisms.
    
keywords: >
    eye movements; language processing; information-seeking; word learning; gaze following
    
output: cogsci2016::cogsci_paper
header-includes:
  - \usepackage{threeparttable}
  - \usepackage{booktabs}
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.asp=0.6, fig.align = 'center',
                      fig.crop = F, out.width = "80%",
                      fig.pos = "t", fig.path='figs/',
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)

source(here::here("code/helper_functions/libraries_and_functions.R"))
source(here::here("code/helper_functions/permutation_helpers.R"))
source(here::here("code/helper_functions/ewma_helper_funs.R"))
source(here::here("code/helper_functions/paper_helpers.R"))
bib <- knitcitations::read.bibtex(here::here("writing/manuscript/speed-acc-novel.bib"))

# data_paths
demo_path <- "data/01_participant_logs/"
data_path <- "data/03_processed_data/"
stimuli_path <- "data/00_stimuli_information/analysis_order_sheets"
image_path <- "writing/figures/plots"
```

```{r model-globals}
library(tidybayes)
options (mc.cores=parallel::detectCores()) # Run on multiple cores
set.seed (3875)
```

```{r read stimuli information}
d_gaze_stim <- read_csv(here::here(stimuli_path, "speed-acc-child-gaze-trial-info.csv"), col_types = cols(.default = "c"))
d_adult_ng_stim <- read_csv(here::here(stimuli_path, "speed-acc-adult-ng-trial-info.csv"), col_types = cols(.default = "c"))
d_stim <- bind_rows(mutate(d_gaze_stim, experiment = "kids_gaze"), mutate(d_adult_ng_stim, experiment = "adults_ng"))
```

```{r read_familiar_words_data}
d_fam_child <- read_csv(here::here(data_path, "familiar_words/speed_acc_child_gaze_fstshift_tidy.csv")) %>% 
  rename(age_days = age) %>% 
  mutate(age_group = "children")

d_fam_adult <- read_csv(here::here(data_path, "familiar_words/speed_acc_adult_ng_fstshift_tidy.csv")) %>% 
  filter(noise_condition == "clear") %>% 
  rename(run_date = dot,
         comments = notes) %>% 
  select(-age, -resp_onset_type_fact, -reason_excluded) %>% 
  mutate(birthday = NA,
         age_group = "adults", 
         age_days = NA,
         run_date = lubridate::as_datetime(run_date))

# clean up and join kid and adult data
d_fam <- bind_rows(d_fam_child, d_fam_adult)
```

```{r read_speed_novel_data}
d_novel_fs <- read_csv(here::here(data_path, "novel_words/speed_acc_novel_shifts.csv")) %>% 
  mutate(learn_block_half = ifelse(trial_num_learn_block <= 2, "first", "second"),
         trial_num_lblock_8 = case_when(
           trial_type == "exposure" & trial_num_learn_block == 1 ~ "1",
           trial_type == "test" & trial_num_learn_block == 1 ~ "2",
           trial_type == "exposure" & trial_num_learn_block == 2 ~ "3",
           trial_type == "test" & trial_num_learn_block == 2 ~ "4",
           trial_type == "exposure" & trial_num_learn_block == 3 ~ "5",
           trial_type == "test" & trial_num_learn_block == 3 ~ "6",
           trial_type == "exposure" & trial_num_learn_block == 4 ~ "7",
           trial_type == "test" & trial_num_learn_block == 4 ~ "8",
           TRUE ~ "NA"),
         correct_bool = ifelse(shift_accuracy == "correct", TRUE, FALSE),
         correct_num = ifelse(shift_accuracy == "correct", 1, 0),
         log_rt = log(rt),
         age_category = ifelse(age_category == "child", "children", "adults")) %>% 
  mutate(age_category = factor(age_category, levels = c("children", "adults")),
         gaze_on_trial = case_when(
           gaze_condition == "gaze" & trial_type == "exposure" ~ "gaze",
           TRUE ~ "no_gaze"
         )
  )

d_novel_tc <- read_feather(here::here(data_path, 
                                      "novel_words/speed_acc_novel_timecourse.feather")) %>% 
  mutate(learn_block_half = ifelse(trial_num_learn_block <= 2, "first", "second"),
         trial_num_lblock_8 = case_when(
           trial_type == "exposure" & trial_num_learn_block == 1 ~ "1",
           trial_type == "test" & trial_num_learn_block == 1 ~ "2",
           trial_type == "exposure" & trial_num_learn_block == 2 ~ "3",
           trial_type == "test" & trial_num_learn_block == 2 ~ "4",
           trial_type == "exposure" & trial_num_learn_block == 3 ~ "5",
           trial_type == "test" & trial_num_learn_block == 3 ~ "6",
           trial_type == "exposure" & trial_num_learn_block == 4 ~ "7",
           trial_type == "test" & trial_num_learn_block == 4 ~ "8",
           TRUE ~ "NA"
         ))
```

# Introduction

People use language to talk about many things, with no guarantee that they refer to objects in the co-occurring visual context. The flexibility of language creates a challenge where learners often encounter new words whose intended meanings are mostly unconstrained. But children are quite capable word learners. How does word learning unfold despite ambiguity in the input? 

Research on lexical development has pursued several solutions.  Lab-based studies and computational models show that learners can overcome referential uncertainty within a labeling event by tracking the elements of a context that remain consistent across multiple exposures to a new word (cross-situational learning) [@yu2007rapid; @roy2002learning]. Social-pragmatic accounts highlight research showing that children's social partners reduce the complexity of the learning task by using gesture and eye gaze to coordinate language interactions with children [@estigarribia2007getting; @bloom2002children]. Moreover, even 16-month-olds can use cues from other people (e.g., the direction of their gaze) to infer the meaning of a new word [@baldwin1993infants].

Both social and statistical information can reduce uncertainty about reference during language processing. These processes, however, do not operate in isolation, and a sophisticated learner might integrate information from both sources to learn words. Computational models show better learning by integrating social information with cross-situational learning mechanisms [@yu2007unified; @frank2009using].

The statistical and social accounts of word learning reviewed above reflect a somewhat passive construal of the learner. Children, however, take actions to shape learning (e.g., choosing where to look or point). Empirical work shows that "active" control can speed learning because it allows people to integrate prior knowledge and uncertainty to seek more useful information [@gureckis2012self]. For example, @kachergis2013actively  found that adults who selected objects to be labeled learned more word-object associations compared to adults who passively experienced labels. 

Here, we pursue an active, integrative account and ask how children's social information seeking changes as they experience multiple exposures to a word-object link.  Our second goal is to test whether children and adults would show a similar pattern of adaptation to gather a social cue to reference. It could be that children rely more on information from social partners because they have to understand and learn language despite having partial knowledge of word-object links and without a fully-developed language model. Moreover, adults have stronger statistical word learning skill, which could diminish the usefulness of seeking a social cue to reference when they detect a consistent association between a word and object. 

In our experiments, we analyze fixations to a speaker's face who either does or does not provide a valid cue to reference: eye gaze.  We chose this behavior as a case study of social information seeking because gaze following is thought to be relevant for the ecological task of linking language to the world. Moreover, recent work has found a reliable link between sustained visual attention on objects and word learning [@smith2013visual]. In Experiment 1, we explore social information seeking in the context of processing concrete, familiar words, where we assume prior exposure to statistical information about word-object links outside the lab. Experiment 2 extends our approach to the case of novel word learning, allowing us to control the number of exposure to a new word and ask whether social information seeking becomes more useful in contexts with more uncertainty.

We characterize eye movements as a series of information seeking decisions that aim to minimize uncertainty [@hayhoe2005eye]. Using this account, children should integrate statistical and social information by considering the utility of an eye movement for their current goal. We hypothesized that when a communicative partner produces a social cue to reference, they increase the value of looking to the speaker for the task of disambiguating reference. Our key behavioral prediction is that listeners will delay generating an eye movement away from a speaker until they have gathered information about the direction of their gaze. This delay will lead to an increase in fixations to the named object, which, in turn, could change the time course of statistical word learning. 

```{r gaze-stimuli, fig.env = "figure*", fig.pos = "h", fig.width=5, fig.asp = 0.3, out.width= "65%", fig.cap = "Stimuli for Experiments 1 and 2. Panel A shows the structure of the linguistic stimuli for a single trial. Panel B shows the layout of the fixation locations for all tasks: the center stimulus, the target, and the distracter. Panel C shows a sample of the images used as novel objects in Experiment 2. Panel D shows an example of the social gaze manipulation."}

knitr::include_graphics(here::here(image_path, "gaze_stimuli.jpeg"))
```

# Experiment 1

In Experiment 1, we measured the time course of children and adults' decisions about visual fixation as they processed sentences with familiar words ("Where's the ball?"). We manipulated whether the speaker produced a post-nominal gaze cue to the named object. The visual world consisted of three fixation targets (a center video of a person speaking, a target picture, and a distracter picture; see Figure 1). The primary question of interest is whether listeners would delay shifting their gaze away from the speaker's face when she was likely to generate a gaze cue. We predicted that fixating longer on the speaker would allow listeners to gather more language-relevant visual information to facilitate comprehension. In contrast, if listeners show parallel gaze dynamics across the gaze and no-gaze conditions, this pattern suggests that hearing the familiar word was the primary factor driving shifts in visual attention.

## Analytic approach

To quantify evidence for our predictions, we present two analyses. First, we analyze the time course of listeners' looking to each area of interest (AOI). Proportion looking reflects the mean proportion of trials on which participants fixated on the speaker, the target image, or the distracter image at every 33-ms interval of the stimulus sentence. We tested condition differences in the proportion looking to the language source -- signer or speaker -- using a nonparametric cluster-based permutation analysis, which accounts for the issue of taking multiple comparisons across many time bins in the timecourse [@maris2007nonparametric]. A higher proportion of looking to the language source in the gaze condition would indicate listeners' prioritization of seeking visual information from the speaker.

Next, we analyzed the RT and Accuracy of participants' initial gaze shifts away from the speaker to objects in the scene. RT corresponds to the latency of shifting gaze away from the central stimulus to either object measured from the onset of the target noun. All reaction time distributions were trimmed to between zero and two seconds, and RTs were modeled in log space. Accuracy corresponds to whether participants' first gaze shift landed on the target or the distracter object. If listeners generate slower but more accurate gaze shifts, this provides evidence that gathering more visual information from the speaker led to more robust language processing in the gaze context.

We used the `brms` [@burkner2017brms] package to fit Bayesian mixed-effects regression models. The mixed-effects approach allowed us to model the nested structure of our data -- multiple trials for each participant and item, and the within-participants manipulation. We used Bayesian estimation to quantify uncertainty in our point estimates, which we communicate using a 95% Highest Density Interval (HDI), providing a range of credible values given the data and model.

## Methods

### Participants

```{r exclusions-table}
# keep_et means too few trials
n_exclude_et_e1 <- d_fam %>% 
  select(subid, keep_et, age_group) %>% 
  unique() %>% 
  count(keep_et, age_group)

n_exclude_runsheet_e1 <- d_fam %>% 
  select(subid, keep_runsheet, age_group) %>% 
  mutate(keep_runsheet = ifelse(keep_runsheet == "keep" | keep_runsheet == "yes", "keep", "drop")) %>% 
  unique() %>% 
  count(keep_runsheet, age_group)

total_exc_children <- n_exclude_et_e1$n[2] + n_exclude_runsheet_e1$n[2]
total_exc_adults <- n_exclude_et_e1$n[1] + n_exclude_runsheet_e1$n[1]
```

```{r make-ss-table, results="asis"}
d_kids <- d_novel_fs %>% 
  filter(age_category == "children", keep_drop == "keep") %>% 
  select(subid, age_days, gender) %>% 
  mutate(Experiment = "Exp. 2 (novel words)") %>% 
  unique()

d_kids %<>% bind_rows(
  d_fam %>% 
    filter(age_group != "adult", keep_et == "include", keep_runsheet == "keep") %>% 
    select(subid, age_days, gender) %>% 
    unique() %>% 
    mutate(Experiment = "Exp. 1 (familiar words)" )
)

gender_breakdown <- d_kids %>% 
  count(Experiment, gender)

kids_age_table <- d_kids %>% 
  mutate(age_months = age_days / 30.25,
         age_years = age_days / 365.25) %>% 
  group_by(Experiment) %>% 
  summarise(n = n(),
            Mean = round(mean(age_months, na.rm = T), 1),
            Min = min(age_months, na.rm = T),
            Max = max(age_months, na.rm = T))

kids_age_table[, -1] <- papaja::printnum(kids_age_table[, -1])

apa_table(
  kids_age_table
  , caption = "Age distributions of children in Experiments 1 and 2. All ages are reported in months."
)
```

```{r adults-demo}
n_adults_gender <- d_novel_fs %>% 
  filter(age_category == "adults", keep_drop == "keep") %>% 
  select(subid, gender) %>% 
  unique() %>% 
  count(gender) %>% 
  mutate(Experiment = "Experiment 3")

n_adults_gender %<>% bind_rows(
  d_fam %>% 
    filter(age_group == "adults", keep_et == "include") %>% 
    select(subid, gender) %>% 
    unique() %>% 
    count(gender) %>% 
    mutate(Experiment = "Experiment 1")
)

n_adults_total <- n_adults_gender %>% group_by(Experiment) %>% summarise(n = sum(n))
```

Participants were native, monolingual English-learning children ($n=$ `r kids_age_table$n[1]`; `r gender_breakdown$n[1]` F) and adults ($n=$ `r n_adults_total$n[1]`; `r n_adults_gender$n[3]` F). All participants had no reported history of developmental or language delay and normal vision. `r total_exc_adults + total_exc_children` participants (`r total_exc_children` children, `r total_exc_adults` adults) were run but not included in the analysis because either the eye tracker falied to calibrate (`r n_exclude_runsheet_e1$n[2]` children, `r n_exclude_runsheet_e1$n[1]` adults) or the participant did not complete the task (`r n_exclude_et_e1$n[2]` children, `r n_exclude_et_e1$n[1]` adults). 

### Materials

```{r linguistic stimuli length}
d_word_lengths <- d_stim %>% 
  filter(!is.na(noun)) %>% 
  select(noun, carrier, noun_onset_sec:noun_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("noun_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (noun_offset_sec * 33) + noun_offset_frames ) - ( (noun_onset_sec * 33) + (noun_onset_frames) ),
    length_ms = length_frames * 33
  ) 

ms_words_length <- d_word_lengths %>% 
  group_by(carrier, noun) %>% 
  summarise(m_length = mean(length_ms)) %>% 
  group_by(noun) %>% 
  summarise(m = mean(m_length) %>% round(digits = 2)) 

## compute gaze length
d_gaze_length <- d_stim %>% 
  filter(!is.na(gaze_onset_sec)) %>% 
  select(noun, carrier, gaze_onset_sec:gaze_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("gaze_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (gaze_offset_sec * 33) + gaze_offset_frames ) - ( (gaze_onset_sec * 33) + (gaze_onset_frames) ),
    gaze_length_ms = length_frames * 33
  ) 

ms_gaze_length <- d_gaze_length %>% 
  summarise(m_gaze = mean(gaze_length_ms) / 1000)

```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (target word)" or "Look! Where's the (target word). The target words were: ball, bunny, boat, bottle, cookie, juice, chicken, and shoe. The target words varied in length (shortest = `r min(ms_words_length$m)` ms, longest = `r max(ms_words_length$m)` ms) with an average length of `r mean(ms_words_length$m) %>% round(digits=2)` ms. 

*Gaze manipulation*. To create the stimuli in the gaze condition, the speaker waited until she finished producing the target sentence and then turned her head to gaze at the bottom right corner of the camera frame. After looking at the named object, she then returned her gaze to the center of the frame. We chose to allow the length of the gaze cue to vary to keep the stimuli naturalistic. The average length of gaze was `r papaja::printnum(ms_gaze_length$m_gaze)` seconds with a range from `r papaja::printnum(min(d_gaze_length$gaze_length_ms) / 1000)` to `r papaja::printnum(max(d_gaze_length$gaze_length_ms) / 1000)` seconds.

*Visual stimuli.* The image set consisted of colorful digitized pictures of objects presented in fixed pairs with no phonological overlap between the target and the distracter image (cookie-bottle, boat-juice, bunny-chicken, shoe-ball). The side of the target picture was counterbalanced across trials.

### Procedure

Participants viewed the task on a screen while their gaze was tracked using an SMI RED corneal-reflection eye-tracker mounted on an LCD monitor, sampling at 30 Hz. The eye-tracker was first calibrated for each participant using a 6-point calibration. On each trial, participants saw two images of familiar objects on the screen for two seconds before the center stimulus appeared. Next, they processed the target sentence -- which consisted of a carrier phrase, a target noun, and a question -- followed by two seconds without language to allow for a response. Both children and adults saw 32 trials (16 gaze trials; 16 no-gaze trials) with several filler trials interspersed to maintain interest. The gaze manipulation was presented in a blocked design with the order of block counterbalanced across participants.

## Results and Discussion

### Timecourse looking

We first analyzed how the presence of gaze influenced listeners' distribution of attention across the three fixation locations while processing familiar words. At target-noun onset, listeners tended to look more at the speaker than the objects. As the target noun unfolded, the mean proportion looking to the center decreased as participants shifted their gaze to the images. Proportion looking to the target increased sooner and reached a higher asymptote compared to proportion looking to the distracter for both gaze conditions with adults spending more time looking at the target compared to children. After looking to the named referent, listeners tended to shift their gaze back to the speaker's face. 

```{r speed-acc-gaze-results, fig.env = "figure*", fig.pos = "t", fig.width=5, fig.asp = 0.6, out.width= "85%", fig.cap = "Timecourse looking, first shift Reaction Time (RT), and Accuracy results for children and adults in Experiment 1. Panel A shows the overall looking to the center, target, and distracter stimulus for each gaze condition and age group. Panel B shows the distribution of pairwise contrasts between each participant's RT in the gaze and no-gaze conditions. The square point represents the group means. The vertical dashed line represents the null model of zero condition difference. Error bars represent the 95\\% HDI. Panel C shows the same information but for first shift accuracy."}

knitr::include_graphics(here::here(image_path, "speed_acc_fam_behav.jpeg"))
```

We did not see evidence that the presence of a post-nominal gaze cue changed how children or adults allocated attention early in the target word. Children in the gaze condition, however, tended to shift their focus back to the speaker earlier after shifting gaze to the named object and spent more time fixating on the speaker's face throughout the rest of the trial ($p < .001$). Next, we ask how these different processing contexts changed the timing and accuracy of children's initial decisions to shift away from the center stimulus.

```{r filter for blmm}
d_model_fam <- d_fam %>% 
  filter(keep_runsheet %in% c("yes", "keep"), 
         keep_et == "include",
         rt <= 2,
         response_onset_type == "noun",
         shift_start_location == "center") %>% 
  mutate(correct_num = ifelse(shift_accuracy_clean == "correct", 1, 0))
```

```{r blmm speed-acc-fam, include = FALSE}
# RT
fit_rt_fam <- brms::brm(log(rt) ~ gaze_condition + age_group + (gaze_condition + target_image|subid), 
                        data = d_model_fam, 
                        family = gaussian(),
                        silent = TRUE,
                        file = here::here('data/04_model_outputs/speed_acc_nov_fit_rt_fam'))

params <- rownames(fixef(fit_rt_fam))

coefs_rt_fam <- fixef(fit_rt_fam) %>% 
  as_tibble() %>% 
  mutate(param = params) %>% 
  mutate_if(is.numeric, printnum) 

ms_rt_fam <- d_model_fam %>% 
  group_by(age_group, gaze_condition) %>% 
  summarise(m = mean(rt * 1000) %>% round(3))

# accuracy
fit_acc_fam <- brms::brm(correct_num ~ gaze_condition + age_group + (gaze_condition + target_image|subid), 
                         data = d_model_fam, 
                         family = binomial(link = 'logit'),
                         silent = TRUE,
                         file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_fam'))

coefs_acc_fam <- fixef(fit_acc_fam) %>% 
  as_tibble() %>% 
  mutate(param = params) %>% 
  mutate_if(is.numeric, printnum) 

ms_acc_fam <- d_model_fam %>% 
  group_by(age_group, gaze_condition) %>% 
  summarise(m = mean(correct_num) %>% round(2))
```

### First shift RT and Accuracy

To quantify differences across the groups, we fit a Bayesian linear mixed-effects regression predicting first shift RT as a function of gaze condition and age group: *Log(RT) $\sim$ gaze condition + age group +  (gaze_condition + item | subject)*. Both children and adults generated similar RTs in the gaze (children $M_{rt}$ = `r ms_rt_fam$m[3]` ms, adults $M_{rt}$ = `r ms_rt_fam$m[1]` ms) and no-gaze (children $M_{rt}$ = `r ms_rt_fam$m[4]` ms, adults $M_{rt}$ = `r ms_rt_fam$m[2]` ms) conditions, with the null value of zero condition differences falling within the 95% credible interval ($\beta$ = `r coefs_rt_fam$Estimate[2]`, 95% HDI [`r coefs_rt_fam$Q2.5[2]`, `r  coefs_rt_fam$Q97.5[2]`]). Next, we fit the same model to estimate first shift accuracy. Adults generated more accurate gaze shifts ($M$ = `r ms_acc_fam$m[1]`) compared to children ($M$ = `r ms_acc_fam$m[3]`) with the null value falling outside the 95% HDI ($\beta_{age}$ = `r coefs_acc_fam$Estimate[3]`, 95% HDI [`r coefs_acc_fam$Q2.5[3]`, `r coefs_acc_fam$Q97.5[3]`]). Similar to the RT analysis, we did not find strong evidence of a difference in performance across the gaze conditions ($\beta$ = `r coefs_acc_fam$Estimate[2]`, 95% HDI [`r coefs_acc_fam$Q2.5[2]`, `r coefs_acc_fam$Q97.5[2]`]).

The time course and first shift analyses suggest that hearing a familiar noun was sufficient for both adults and children to shift visual attention away from the speaker and seek a named referent. Neither age group showed evidence of delaying eye movements to gather a social cue to reference. Children did allocate more attention to the speaker after processing the familiar noun. While we did not predict these results, listeners' behavior seems reasonable if eye movements during familiar language processing are highly-practiced visual routines such that seeking a post-nominal gaze cue becomes less-relevant to disambiguating reference. 

The results of Experiment 1 suggest that listeners do not always seek social information when it is available; instead, they might take their uncertainty into account and use social information when ambiguity is higher. This interpretation raises an interesting question: Would listeners adapt to gather social information when they do not already know the meaning of a word? That is, when surrounded by unfamiliar objects, the value of fixating on a social partner may increase since this action could provide access to critical disambiguating information such as eye gaze or pointing -- an idea emphasized by social-pragmatic theories of language acquisition [@bloom2002children]. Experiments 2 explores this case to ask whether learners adapt the timing of gaze shifts to seek information from social partners when encountering a novel word.

# Experiment 2

In Experiment 2, we ask three questions: (1) do listeners adapt their gaze to seek social information in the context of processing novel words ? (2) Does social information seeking change as a function of gaining more exposures to a word-object link? And (3) does following a gaze cue change the link between attention during labeling and word learning? To answer these questions, we compared the timing and accuracy of eye movements during a real-time cross-situational word learning task where participants processed sentences containing a novel word ("Where's the dax?") while looking at a simplified visual world.

We predicted that the presence of gaze would increase the value of looking to a speaker, leading to a higher proportion of fixations to the social target and slower first shift reaction times to the objects, especially earlier in learning. We operationalize this prediction as a main effect of gaze condition on proportion looking to the speaker, first shift RT, and a trial number by gaze condition interaction such that the decrease in RT will be greater on exposure trials in the gaze condition. We also predicted that the presence of gaze would lead to faster learning of the novel word-object links, which we operationalized as more accurate first shifts, faster RTs, and a higher proportion looking to the target object on test trials in the gaze condition.

```{r san-prop-looking-plot, fig.env = "figure*", fig.pos = "t", fig.width=4, fig.asp = 0.5, out.width= "85%", fig.cap = "Panel A shows participants’ tendency to look at the speaker on exposure and test trials as a function of the trial number within a learning block. The horizontal, dashed line represents the tendency to distribute attention equally across the three AOIs. Color indicates gaze condition and error bars represent 95\\% credible intervals. Panel B shows the same information but for target and distracter looking across the learning block (left) and aggregated over all trials (right)."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_proplook.jpeg"))
```

## Methods

### Participants

```{r filter timecourse data}
d_analysis_nov <- d_novel_tc %>% 
  filter(keep_drop == "keep",
         t_rel_noun >= 0, 
         t_rel_noun <= 3)

d_analysis_nov %<>% 
  split(.$subid) %>%  
  purrr::map_dfr(create_time_bins_ss, t_ms_diff = 33)

# exclude based on runsheet information
d_exclusions_rs <- d_novel_tc %>% 
  distinct(subid, keep_drop, reason_excluded, age_category) %>% 
  filter(keep_drop == "drop") 

# exclude based on number of trials completed.
d_exclusions_et <- d_analysis_nov %>%
  filter(target_looking != "away") %>% 
  distinct(subid, trial_num_exp, age_category) %>% 
  dplyr::count(subid, age_category) %>% 
  filter(n < 16) %>% 
  mutate(keep_drop = "drop", 
         reason_excluded = "fewer than half trials completed") 

# create subid vector for filtering later
ss_exclude <- d_exclusions_et %>% pull(subid)

# Join exclusions tables and save to disk.
d_exclusions_final <- d_exclusions_et %>% 
  dplyr::select(-n) %>% 
  bind_rows(d_exclusions_rs)

d_analysis_nov %<>% filter(!(subid %in% ss_exclude))

## flag trials with less than half looking to AOIs
trial_filter <- d_analysis_nov %>% 
  distinct(subid, trial_num_exp, learning_block, gaze_condition, target_looking, time_ms_normalized) %>% 
  count(subid, trial_num_exp, learning_block, gaze_condition, target_looking) %>% 
  group_by(trial_num_exp, subid) %>% 
  mutate(total_samples = sum(n),
         prop_trial = n / total_samples,
         good_trial = case_when(
           target_looking == "away" & prop_trial >= 0.5 ~ "drop",
           TRUE ~ "keep"
         )) %>% 
  distinct(subid, trial_num_exp, good_trial)

d_analysis_nov %<>% left_join(trial_filter)

n_samples_trial <- d_analysis_nov %>% 
  count(subid, trial_num_exp) %>% 
  rename(n_samples_trial = n)

d_analysis_nov %<>% left_join(n_samples_trial)

d_analysis_nov %<>% 
  filter(target_looking != "away", good_trial == "keep") %>% 
  mutate(age_category = ifelse(age_category == "child", "children", "adults")) %>% 
  mutate(age_category = factor(age_category, levels = c("children", "adults")))
```

```{r exclusions-e3}
d_excluded_e3 <- d_exclusions_final %>% count(reason_excluded, age_category)
```

Participants were native, monolingual English-learning children ($n=$ `r kids_age_table$n[2]`; `r gender_breakdown$n[3]` F) and adults ($n=$ `r n_adults_total$n[2]`; `r n_adults_gender$n[1]` F). All participants had no reported history of developmental or language delay and normal vision. `r d_excluded_e3$n[2]` adults were run but not included in the analysis because they were not native speakers of English. `r d_excluded_e3$n[1]` children participants were run but not included in the analysis because the participant did not complete more than half of the trials in the task.

### Materials

```{r speed-acc-nov-stim}
d_nov_stim <- read_csv(here::here("data/00_stimuli_information/analysis_order_sheets/speed-acc-novel-analysis-order-sheet.csv"))

# compute length of words
nov_word_stim <- d_nov_stim %>% 
  select(stimulus_name, noun_onset_sec, noun_onset_frames, noun_offset_sec, noun_offset_frames) %>% 
  unique() %>% 
  mutate(noun_onset_ms = (noun_onset_sec * 1000) + (noun_onset_frames * 33),
         noun_offset_ms = (noun_offset_sec * 1000) + (noun_offset_frames * 33),
         noun_length_ms = noun_offset_ms - noun_onset_ms
  ) %>% 
  summarise(m = mean(noun_length_ms), 
            min_word = min(noun_length_ms),
            max_word = max(noun_length_ms))

# compute length of gaze
nov_gaze_stim <- d_nov_stim %>% 
  select(stimulus_name, gaze_onset_sec, gaze_onset_frames, gaze_offset_sec, gaze_offset_frames) %>% 
  unique() %>% 
  filter(!is.na(gaze_onset_sec)) %>% 
  mutate(gaze_onset_ms = (gaze_onset_sec * 1000) + (gaze_onset_frames * 33),
         gaze_offset_ms = (gaze_offset_sec * 1000) + (gaze_offset_frames * 33),
         gaze_length_ms = gaze_offset_ms - gaze_onset_ms
  ) %>% 
  summarise(m = mean(gaze_length_ms), 
            min_gaze = min(gaze_length_ms),
            max_gaze = max(gaze_length_ms))
```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (novel word)" or "Look! Where's the (novel word). The target words were four pseudo-words: bosa, modi, toma, and pifo. The novel words varied in length (shortest = `r papaja::printnum(nov_word_stim$min_word)` ms, longest = `r papaja::printnum(nov_word_stim$max_word)` ms) with an average length of `r papaja::printnum(nov_word_stim$m)` ms. 

*Gaze manipulation*. The gaze manipulation was identical to Experiment 1. The average length of gaze was `r papaja::printnum(nov_gaze_stim$m / 1000)` seconds with a range from `r papaja::printnum(nov_gaze_stim$min_gaze / 1000)` to `r papaja::printnum(nov_gaze_stim$max_gaze / 1000)` seconds.

*Visual stimuli.* The image set consisted of 28 colorful digitized pictures of objects that were selected such that they would be interesting to and that children would be unlikely to have already a label associated with the objects. The side of the target picture was counterbalanced across trials.

### Procedure

The procedure was identical to Experiment 1. Participants watched a series of ambiguous word learning events organized into pairs of one exposure and one test trial. On each trial, participants saw of a set of two unfamiliar objects and heard one novel word. Each word occurred in a block of four exposure-test pairs for a total of eight trials for each novel word. On each trial within a learning block, one of the objects in the set had consistently appeared on the previous trials (target object), while the other object was a randomly generated novel object that had not been shown in the experiment (distracter object). Both children and adults saw 32 trials (16 gaze trials; 16 no-gaze trials) with several filler trials interspersed to maintain interest. The gaze manipulation was presented in a blocked design with the order of block counterbalanced across participants.

## Results and Discussion

<!-- ### Timecourse looking -->

<!-- ```{r san-tc-plot, include = F, fig.env = "figure*", fig.pos = "t", fig.width=5, fig.asp = 0.6, out.width= "85%", fig.cap = "Overview of children and adults' looking to the three fixation targets (Speaker, Target, Distracter) over the course of exposure and test trials. Panel A shows proportion looking to the speaker's face with color indicating gaze condition and line type indicating age group. Panel B shows the same information but for proportion looking to the target and distracter images."} -->

<!-- knitr::include_graphics(here::here(image_path, "speed_acc_novel_tc.jpeg")) -->
<!-- ``` -->

<!-- *Looking to the speaker.* How did the presence of a gaze cue change learners' decisions to fixate on the speaker? Visual inspection of Figure \ref{fig:san-tc-plot}A shows that both children and adults tended to start looking at the speaker at noun onset and shifted their gaze away as the noun unfolded, with adults doing so sooner compared to children. On exposure trials when there was a gaze cue, both adults and children tended to look more to the face at noun onset as indicated by the higher intercept of the blue curves. Moreover, around one second after noun onset, listeners tended to shift their attention back to the speaker's face more often and especially so for children. This pattern of looking parallels the effect of gaze on children's time course of fixations while processing familiar words in Experiment 1. On test trials that were preceded by an exposure trial with gaze, children and adults tended to look more to the speaker even though there was no gaze cue present. This pattern suggests that the presence of gaze likely modulated learners' expectations of gathering disambiguating information from the speaker on test trials. -->

<!-- *Looking to the target and distracter.* Next, we asked how learners divided attention between the target and distracter objects. On exposure trials, looking to both objects increased throughout the trial but more so for looks to the named object as indicated by the higher asymptote of the target looking curves. Adults spent more time looking to the target and less time looking to the distracter as compared to children. Interestingly, when there was a gaze cue to process, children and adults allocated fewer fixations to the distracter object, providing evidence that social information could reduce the potential for learners to create spurious word-object links during statistical word learning.  -->

### Proportion looking

```{r aggregate prop looking}
ss_groupings <- list("subid", "gaze_condition", "trial_type", "age_category", "trial_num_learn_block", "trial_num_exp")
ms_groupings <- list("subid", "gaze_condition", "trial_type", "age_category", "trial_num_learn_block")

ss_novel_face <- d_analysis_nov %>% 
  aggregate_ss_looking(df = ., 
                       ss_groupings, 
                       ms_groupings, 
                       aoi_column = "target_looking", 
                       return_ss_df = TRUE) 

ss_novel_obj <- d_analysis_nov %>% 
  filter(target_looking != "center") %>% 
  aggregate_ss_looking(df = ., 
                       ss_groupings, 
                       ms_groupings, 
                       aoi_column = "target_looking", 
                       return_ss_df = TRUE) 
```

```{r fit prop looking speaker blmm, include = FALSE}
fit_proplook_face_nov <- ss_novel_face %>% 
  filter(target_looking == "center", trial_type == "exposure") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + age_category + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block | subid),
            family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_face')
  )

params_proplook_face <- rownames(fixef(fit_proplook_face_nov))

coefs_proplook_face_nov <- fixef(fit_proplook_face_nov) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_face) %>% 
  mutate_if(is.numeric, round, 2) 
```

```{r fit target looking compared to chance, include = FALSE}
fit_chance <- ss_novel_obj %>% 
  filter(target_looking == "target", trial_type == "test") %>% 
  brms::brm(data = ., 
            prop_looking ~ gaze_condition + age_category + (gaze_condition | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_chance')
  )

coefs_chance_nov <- fit_chance %>% 
  spread_draws(b_gaze_conditionstraight_ahead, b_Intercept, b_age_categoryadults) %>%
  mutate(no_gaze_adults = b_Intercept + b_gaze_conditionstraight_ahead + b_age_categoryadults, 
         gaze_adults = b_Intercept + b_age_categoryadults,
         no_gaze_kids = b_Intercept + b_gaze_conditionstraight_ahead) %>%
  rename(gaze_kids = b_Intercept) %>% 
  select(.draw, gaze_kids, no_gaze_adults:no_gaze_kids) %>% 
  gather(param, value, -.draw) %>% 
  group_by(param) %>% 
  summarise(m = quantile(value, probs = 0.5),
            ci_lower = quantile(value, probs = 0.05),
            ci_upper = quantile(value, probs = 0.95))
```

```{r fit prop looking target blmm, include = FALSE}
fit_proplook_target_nov_all <- ss_novel_obj %>% 
  filter(target_looking == "target") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + trial_type + age_category + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block + trial_type | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_targ_all')
  )

params_proplook_targ <- rownames(fixef(fit_proplook_target_nov_all))

coefs_proplook_targ_nov <- fixef(fit_proplook_target_nov_all) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_targ) %>% 
  mutate_if(is.numeric, round, 2) 

## fit separate models for the adults and kids
## this is totally exploratory and just fit to answer 
## the question of whether we could measure learning in kids, would we
## expect to see an effect of the gaze manipulation on learning trajectories

fit_proplook_target_nov_adults <- ss_novel_obj %>% 
  filter(target_looking == "target", age_category == "adults") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + trial_type + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block + trial_type | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_targ_adults')
  )

params_proplook_targ_adults <- rownames(fixef(fit_proplook_target_nov_adults))

coefs_proplook_targ_nov_adults <- fixef(fit_proplook_target_nov_adults) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_targ_adults) %>% 
  mutate_if(is.numeric, round, 2) 

## Kids model
fit_proplook_target_nov_kids <- ss_novel_obj %>% 
  filter(target_looking == "target", age_category == "children") %>% 
  brms::brm(data = ., 
            prop_looking ~ (gaze_condition + trial_type + trial_num_learn_block)^2 + 
              (gaze_condition + trial_num_learn_block + trial_type | subid)
            , family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_proplook_targ_kids')
  )

params_proplook_targ_kids <- rownames(fixef(fit_proplook_target_nov_kids))

coefs_proplook_targ_nov_kids <- fixef(fit_proplook_target_nov_kids) %>% 
  as_tibble() %>% 
  mutate(param = params_proplook_targ_kids) %>% 
  mutate_if(is.numeric, round, 2) 

## get summary stats for prop looking to target
ss_novel_obj %>% 
  filter(target_looking == "target") %>% 
  group_by(age_category, trial_type, gaze_condition) %>% 
  summarise(m = mean(prop_looking))
```

*Learning effects.* Both children ($M_{gaze}$ = `r papaja::printnum(coefs_chance_nov$m[2])`, $M_{no-gaze}$ = `r papaja::printnum(coefs_chance_nov$m[4])`) and adults ($M_{gaze}$ = `r papaja::printnum(coefs_chance_nov$m[1])`, $M_{no-gaze}$ = `r papaja::printnum(coefs_chance_nov$m[3])`) showed evidence of learning the novel word-object links, with the null value of 0.5 falling below the lower bound of the lowest credible interval for children's target looking in the No-gaze context (95% HDI [`r papaja::printnum(coefs_chance_nov$ci_lower[4])`, `r papaja::printnum(coefs_chance_nov$ci_upper[4])`]). Our primary question of interest was how exposure to multiple co-occurrences of word-object pairs would change learners' distribution of attention between the speaker and objects. Figure \ref{fig:san-prop-looking-plot} shows proportion looking to the speaker and the target and distracter objects as a function of trial number within a word learning block. Both children and adults were more likely to fixate on the speaker when she provided a gaze cue ($\beta_{gaze}$ = `r papaja::printnum(coefs_proplook_face_nov$Estimate[2] * -1)`, 95% HDI [`r papaja::printnum(coefs_proplook_face_nov$Q2.5[2] * -1)`, `r papaja::printnum(coefs_proplook_face_nov$Q97.5[2] * -1)`]). Moreover, there was a developmental difference such that children, but not adults, were more likely to increase their fixations to the speaker over the course of the learning block ($\beta_{age*tr.num}$ = `r papaja::printnum(coefs_proplook_face_nov$Estimate[7])`, 95% HDI [`r papaja::printnum(coefs_proplook_face_nov$Q2.5[7])`, `r papaja::printnum(coefs_proplook_face_nov$Q97.5[7])`]). 

Overall, looking to the target increased as learners were exposed to more word-object pairings ($\beta_{tr.num}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[4])`, 95% HDI [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[4])`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[4])`]) and was higher when the novel word was accompanied by a gaze cue ($\beta_{gaze}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[2] * -1)`, 95% HDI [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[2] * -1)`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[2] * -1)`]). Visual inspection of Figure \ref{fig:san-prop-looking-plot} shows that on the first exposure trial, both adults and children used the gaze cue to disambiguate reference, fixating more on the target in the aze condition. For children, higher target looking on exposure trials with gaze remained relatively constant across the learning block. In contrast, adults target looking reached ceiling for both gaze and no-gaze conditions by trial number two, indicating that they had successfully used the co-occurrence information across trials to map the novel word to its referent. We found an interaction between gaze condition and trial number such that looking to the target increased more quickly in the No-gaze condition ($\beta_{gaze*tr.num}$ = `r papaja::printnum(coefs_proplook_targ_nov$Estimate[8])`, 95% HDI [`r papaja::printnum(coefs_proplook_targ_nov$Q2.5[8])`, `r papaja::printnum(coefs_proplook_targ_nov$Q97.5[8])`]), which reflects (1) the higher intercept of target looking in the presence of gaze and (2) rapid learning of the word-object association via cross-situational information. Finally, visual inspection of the proportion looking plot suggests that adults tended to look more the target when learning from a gaze cue, only reaching similar levels of accuracy in the no-gaze condition at the end of the learning block. There was not strong evidence for an effect of the gaze manipulation on children's looking behavior on Test trials. 

```{r speed-acc-novel exposure-test looking aggregate}
ss_groupings <- list("subid", "gaze_condition", "trial_type", "age_category", "trial_num_learn_block", "target_word",  "trial_num_exp")
ms_groupings <- list("subid", "gaze_condition", "trial_type", "age_category", "trial_num_learn_block", "target_word")

ss_novel_learning <- d_analysis_nov %>% 
  #filter(target_looking != "center") %>% 
  aggregate_ss_looking(df = .,
                       ss_groupings, 
                       ms_groupings, 
                       aoi_column = "target_looking", 
                       return_ss_df = TRUE) 

exposure_ss <- ss_novel_learning %>% 
  filter(target_looking == "target") %>% 
  select(subid, trial_num_exp, trial_num_learn_block, trial_num_exp, 
         target_word, age_category, gaze_condition, trial_type, prop_looking) %>% 
  spread(trial_type, prop_looking) %>% 
  filter(!is.na(exposure)) %>% 
  select(-test, -trial_num_exp)

test_ss <- ss_novel_learning %>% 
  filter(target_looking == "target") %>% 
  select(subid, trial_num_exp, trial_num_learn_block, trial_num_exp, 
         target_word, age_category, gaze_condition, trial_type, prop_looking) %>%  
  spread(trial_type, prop_looking) %>% 
  filter(!is.na(test)) %>% 
  select(-exposure, -trial_num_exp)

ss_learning_final <- left_join(exposure_ss, test_ss)
```

```{r blmm speed-acc-novel exp-test looking, include=FALSE}
fit_exp_test_look <- ss_learning_final %>% 
  brms::brm(data = ., 
            test ~ (exposure + gaze_condition + age_category + trial_num_learn_block)^2 + (gaze_condition + trial_num_learn_block | subid),
            family = gaussian(),
            silent = TRUE,
            file = here::here('data/04_model_outputs/speed_acc_nov_fit_exp_test')
  )

params_exp_test <- rownames(fixef(fit_exp_test_look))

coefs_exp_test <- fixef(fit_exp_test_look) %>% 
  as_tibble() %>% 
  mutate(param = params_exp_test) %>% 
  mutate_if(is.numeric, printnum) 
```

*Relationship between looking on exposure and test.* For both children and adults, more time attending to the target object on exposure trials led to a higher proportion of looking to the target on test trials, especially for adults ($\beta_{exposure*age}$ = `r papaja::printnum(coefs_exp_test$Estimate[7])`, 95% HDI [`r papaja::printnum(coefs_exp_test$Q2.5[7])`, `r papaja::printnum(coefs_exp_test$Q97.5[7])`]) and as the number of word-object exposures increased over the course a learning block ($\beta_{exposure*tr.num}$ = `r papaja::printnum(coefs_exp_test$Estimate[8])`, 95% HDI [`r papaja::printnum(coefs_exp_test$Q2.5[8])`, `r papaja::printnum(coefs_exp_test$Q97.5[8])`]). There was evidence that participants in the No-gaze condition showed less learning over the course of each word block ($\beta_{gaze*tr.num}$ = `r papaja::printnum(coefs_exp_test$Estimate[10])`, 95% HDI [`r papaja::printnum(coefs_exp_test$Q2.5[10])`, `r papaja::printnum(coefs_exp_test$Q97.5[10])`]). This result dovetails with the findings from Experiment 2, providing evidence that the presence of social information did more than change attention on exposure trials but instead modulated the relationship between attention during learning and later memory for word-object links.

The proportion looking analyses suggest that the presence of gaze changed how children and adults allocated attention while processing novel words. In the context of unfamiliar objects, children tended to fixate more on a speaker's face when she provided a post-nominal social cue to reference, a difference in looking behavior that increased as they were exposed to more word-object co-occurrences. This result is different from the parallel looking behavior that we found in Experiment 1 where listeners processed highly familiar nouns. Moreover, in the presence of a speaker who provided a gaze cue, children and adults spent less time fixating on the distracter image, which modulates the word-object connections that learners could store from labeling event. These changes in gaze patterns, however, did not generalize to performance differences on Test trials for children. Finally, we found that the presence of a social cue increased the strength of the link between attention on exposure and fixations at test.

### First shift RT and Accuracy

```{r speed-acc-novel-shifts, include=F, out.width="90%", fig.cap = "First shift Reaction Time (RT), and Accuracy results for children and adults in Experiment 2. Panel A shows the distribution of pairwise contrasts between RTs in the gaze and no-gaze conditions. The square point represents the mean value for each measure. The vertical dashed line represents the null model of zero condition difference. The width each point represents the 95\\% HDI. Panel B shows the same information but for participants' first shift accuracy."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_fstshifts.jpeg"))
```

```{r speed-acc-nov filter for blmm}
d_model_nov <- d_novel_fs %>%
  filter(is.na(block_excluded) | block_excluded != learning_block, 
         shift_start_location == "center") %>%
  group_by(subid, learning_block) %>%
  mutate(n_trials = n()) %>% 
  filter(rt > 0, 
         keep_drop == "keep", 
         n_trials >= 2) %>% 
  ungroup()

m_rt <- log(d_model_nov$rt) %>% mean()
sd_rt <- log(d_model_nov$rt) %>% sd()

d_model_nov %<>% 
  mutate(log_rt = log(rt),
         keep_rt = case_when(
           log_rt <= m_rt + 3*sd_rt & log_rt >= m_rt - 3*sd_rt ~ "keep",
           TRUE ~ "drop"
         )) %>% 
  filter(keep_rt == "keep")
```

```{r blmm rt speed-acc-novel, include = FALSE}
# RT
fit_rt_nov <- brms::brm(log(rt) ~ (gaze_on_trial + age_category + learn_block_half)^2 + 
                          (gaze_on_trial|subid), 
                        data = filter(d_model_nov, trial_type == "exposure"),
                        family = gaussian(),
                        silent = TRUE,
                        file = here::here('data/04_model_outputs/speed_acc_nov_fit_rt_nov_exp'))

params <- rownames(fixef(fit_rt_nov))

coefs_rt_nov <- fixef(fit_rt_nov) %>% 
  as_tibble() %>% 
  mutate(param = params) %>% 
  mutate_if(is.numeric, printnum) 

## summary stats for RT
ms_rt_nov <- d_model_nov %>% 
  filter(keep_rt == "keep", trial_type == "exposure") %>% 
  group_by(age_category, gaze_on_trial, learn_block_half) %>% 
  summarise(m = mean(rt) * 1000)

fit_rt_nov_test <- brms::brm(log(rt) ~ (gaze_on_trial + age_category + learn_block_half)^2 + 
                               (gaze_on_trial|subid), 
                             data = filter(d_model_nov, trial_type == "exposure"),
                             family = gaussian(),
                             silent = TRUE,
                             file = here::here('data/04_model_outputs/speed_acc_nov_fit_rt_nov_test'))

params_rt_test <- rownames(fixef(fit_rt_nov_test))

coefs_rt_nov_test <- fixef(fit_rt_nov_test) %>% 
  as_tibble() %>% 
  mutate(param = params_rt_test) %>% 
  mutate_if(is.numeric, printnum) 
```

```{r blmm accuracy speed-acc-novel, include=FALSE}
# accuracy exposure trials
fit_acc_nov_exp <- brms::brm(correct_num ~ (gaze_on_trial + age_category + learn_block_half)^2 + (gaze_on_trial|subid),
                             data = filter(d_model_nov, trial_type == "exposure"), 
                             family = binomial(link = 'logit'),
                             silent = TRUE,
                             file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_exp'))

params_acc <- rownames(fixef(fit_acc_nov_exp))

coefs_acc_nov_exp <- fixef(fit_acc_nov_exp) %>% 
  as_tibble() %>% 
  mutate(param = params_acc) %>% 
  mutate_if(is.numeric, printnum)

# d_model_nov %>% 
#   filter(keep_rt == "keep", trial_type == "exposure") %>% 
#   group_by(age_category, gaze_on_trial, learn_block_half) %>% 
#   summarise(m = mean(correct_num)) %>% 
#   ggplot(aes(x = learn_block_half, y = m, color = gaze_on_trial)) +
#   geom_point() + 
#   geom_line(aes(group=gaze_on_trial)) +
#   facet_wrap(~age_category)

# accuracy test trials
fit_acc_nov_test <- brms::brm(correct_num ~ (gaze_condition + age_category + trial_num_learn_block)^2 + 
                                (gaze_condition + trial_num_learn_block|subid),
                              data = filter(d_model_nov, trial_type == "test"), 
                              family = binomial(link = 'logit'),
                              silent = TRUE,
                              file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_test'))

params_acc_test <- rownames(fixef(fit_acc_nov_test))

coefs_acc_nov_test <- fixef(fit_acc_nov_test) %>% 
  as_tibble() %>% 
  mutate(param = params_acc_test) %>% 
  mutate_if(is.numeric, printnum)  

# accuracy test trials
fit_acc_nov_test_no_int <- brms::brm(correct_num ~ (gaze_condition + age_category + trial_num_learn_block) + 
                                       (gaze_condition + trial_num_learn_block|subid),
                                     data = filter(d_model_nov, trial_type == "test"), 
                                     family = binomial(link = 'logit'),
                                     silent = TRUE,
                                     file = here::here('data/04_model_outputs/speed_acc_nov_fit_acc_test_noint'))

params_acc_test_noint <- rownames(fixef(fit_acc_nov_test_no_int))

coefs_acc_nov_test_noint <- fixef(fit_acc_nov_test_no_int) %>% 
  as_tibble() %>% 
  mutate(param = params_acc_test_noint) %>% 
  mutate_if(is.numeric, printnum)

# d_model_nov %>% 
#   filter(keep_rt == "keep", trial_type == "test") %>% 
#   group_by(age_category, gaze_condition, trial_num_learn_block) %>% 
#   summarise(m = mean(correct_num)) %>% 
#   ggplot(aes(x = trial_num_learn_block, y = m, color = gaze_condition)) +
#   geom_point() + 
#   geom_line(aes(group=gaze_condition)) +
#   facet_wrap(~age_category)
```

```{r speed-acc-novel summary-stats}
## summary stats for RT
ms_rt_nov <- d_model_nov %>% 
  filter(keep_rt == "keep") %>% 
  group_by(age_category, gaze_on_trial) %>% 
  summarise(m = mean(rt) * 1000) %>% 
  mutate(m = papaja::printnum(m))

## summary stats for accuracy 
ms_acc_nov <- d_model_nov %>% 
  group_by(age_category, gaze_condition, trial_type) %>% 
  summarise(m = mean(correct_num) %>% papaja::printnum())
```

We next asked how the presence of gaze influenced learners' decision to stop gathering visual information from the speaker and start fixating on the novel objects. To quantify the effect the gaze, we fit a Bayesian linear mixed-effects regression predicting first shift RT as a function of whether there was a gaze cue present on the trial and age group. Both children (Gaze $M_{rt}$ = `r ms_rt_nov$m[1]` ms, No-gaze $M_{rt}$ = `r ms_rt_nov$m[2]` ms)  and adults (Gaze $M_{rt}$ = `r ms_rt_nov$m[3]` ms, No-gaze $M_{rt}$ = `r ms_rt_nov$m[4]` ms) fixated longer on the speaker when she provided a gaze cue ($\beta_{gaze}$ = `r coefs_rt_nov$Estimate[2]`, 95% HDI [`r coefs_rt_nov$Q2.5[2]`, `r coefs_rt_nov$Q97.5[2]`]). With no evidence of an interaction between gaze condition and age group  ($\beta_{age*gaze}$ = `r coefs_rt_nov$Estimate[4]`, 95% HDI [`r coefs_rt_nov$Q2.5[4]`, `r coefs_rt_nov$Q97.5[4]`]). Moreover, both (Gaze $M_{acc}$ = `r ms_acc_nov$m[1]`, No-gaze $M_{acc}$ = `r ms_acc_nov$m[3]`)  and adults (Gaze $M_{acc}$ = `r ms_acc_nov$m[5]`, No-gaze $M_{acc}$ = `r ms_acc_nov$m[7]`) generated more accurate first shifts in the gaze condition, indicating they were following the gaze cue on exposure trials ($\beta_{gaze}$ = `r coefs_acc_nov_exp$Estimate[2]`, 95% HDI [`r coefs_acc_nov_exp$Q2.5[2]`, `r  coefs_acc_nov_exp$Q97.5[2]`]). 

Finally, we asked whether the presence of gaze affected learning by predicting first shift accuracy on Test trials. We found that adults were more accurate than children ($\beta_{age}$ = `r coefs_acc_nov_test_noint$Estimate[3]`, 95% HDI [`r coefs_acc_nov_test_noint$Q2.5[3]`, `r  coefs_acc_nov_test_noint$Q97.5[3]`]), that first shifts became more accurate as learners experienced repeated exposures to word-object pairings ($\beta_{tr.num}$ = `r coefs_acc_nov_test_noint$Estimate[4]`, 95% HDI [`r coefs_acc_nov_test_noint$Q2.5[4]`, `r  coefs_acc_nov_test_noint$Q97.5[4]`]). We did not see evidence for two of our predictions: (1) that children and adults would generate more accurate first shifts when learning from social gaze ($\beta_{gaze}$ = `r coefs_acc_nov_test_noint$Estimate[2]`, 95% HDI [`r coefs_acc_nov_test_noint$Q2.5[2]`, `r  coefs_acc_nov_test_noint$Q97.5[2]`]) and (2) that learning from gaze would modulate the relationship between accuracy over the course of learning ($\beta_{gaze*tr.num}$ = `r coefs_acc_nov_test$Estimate[6]`, 95% HDI [`r coefs_acc_nov_test$Q2.5[6]`, `r  coefs_acc_nov_test$Q97.5[6]`]), with the null value falling within each credible interval. 

Returning to our three behavioral predictions, we found evidence that both children and adults spent more time fixating on a speaker when she provided a useful social cue to reference. Moreover, adults decreased the amount of time fixating on the speaker as they gained more exposures to the word-object pairings, but children showed the opposite pattern, increasing their fixations to the speaker later in the task. This developmental difference suggests that looking to a social partner may have been more useful for children who were still trying to disambiguate the novel words; whereas adults showed evidence of successful disambiguation after the second exposure trial and could focus attention on the objects instead. Finally, we found mixed evidence that the presence of gaze modulated the relationship between visual attention during labeling and learning of the novel word-object mappings. Both children and adults generated a higher proportion of shifts landing on the target when there was post-nominal gaze cue available. But only adults spent more time fixating on the target object and generated more accurate first shifts for words learned with a gaze cue.

# General Discussion

Does social information seeking change as children gain more statistical information about the link between a word and object? Here, we pursued the idea that learners flexibly adapt their eye movements to gather social gaze when it was useful. We found that children and adults did not delay their gaze shifts to seek a post-nominal gaze cue while processing familiar words. When processing novel words, however, both children and adults fixated more on a speaker to seek a post-nominal gaze cue. This delay resulted in more attention allocated to the named object and less looking to the distracter object, an effect that increased throughout the task for children. Moreover, adults, but not children, showed evidence of stronger learning in the presence of social gaze while both age groups were capable of learning the word-object pairings from cross-situational statistics alone.

How should we characterize the effects of gaze on information seeking and word learning in our task? Children selectively gathered social information when they were uncertain about the meaning of a new word, focusing attention on a single object. This pattern of behavior generalized to trials without a gaze cue, showing how the effect of gaze could accumulate over time. Finally, seeking social gaze increased the rate of word learning for adults. This finding dovetails with other work showing that the presence of social information changes information processing [@wu2011infants; @yoon2008communication]. 

These studies have several important limitations. First, we did not find strong evidence that the effects of gaze generalized to contexts without gaze for children in Experiment 2. Moreover, children did not show strong learning of the novel word-object links overall. Prior work has shown that 3-5 year-olds learn words better from an extended, as opposed to brief, social cue to reference [@yurovsky2013online]. Future work could increase the length of the gaze cue, which was relatively short in these studies (~2 sec). Second, we used a binary manipulation of the social context -- a fully disambiguating gaze cue or entirely ambiguous label without gaze. These extremes do not reflect the complexity of children's social interactions. For example, observational studies of child-caregiver play sessions show that social cues such as eye gaze or pointing are noisy [@frank2013social] and that caregivers tend to provide a mixture of ambiguous and clear labeling events [@medina2011words]. It would be interesting to know how children's real-time information selection responds to graded changes in the utility of social information for reducing referential ambiguity.

This work integrates social-pragmatic and statistical accounts of language acquisition. We found that listeners' decisions to seek social information varied depending on their uncertainty over word-object mappings. In the context of processing novel, but not familiar words, listeners adapted their gaze to seek a post-nominal social cue to reference. This change led to increased visual attention on a single object and less attention distributed across potential spurious word-object links. Moreover, following gaze modulated the relationship between attention during labeling and learning of word meaning. This approach sheds light on how children might integrate social and statistical information when deciding where to look, which, in turn, shapes the information that comes into contact with statistical learning processes.

\newpage

```{r session info, include = F}
sessionInfo() %>% pander::pander(compact = F)
```

\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering Data/code available at \url{https://bit.ly/2FgIbsW} \\ E1 preregistration at \url{https://osf.io/2q4gw/}\\ E2 preregistration at \url{https://osf.io/nfz85/}}} \vspace{1em}

# Acknowledgements

We are grateful to the people who participated in this research. Thanks to Kayla Constandse, Tami Alade, and Hannah Slater for help with data collection. This work was supported by an NSF GRFP to KM and a Jacobs Foundation Fellowship to MCF.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
