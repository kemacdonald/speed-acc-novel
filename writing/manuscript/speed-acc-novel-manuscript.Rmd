---
title             : "Statistical and social information modulate children's eye movements during language comprehension and word learning"
shorttitle        : "Integrating social and statistical information"

author: 
  - name          : "Kyle MacDonald"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Stanford, CA 94306"
    email         : "kylem4@stanford.edu"
  - name          : "Elizabeth Swanson"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Stanford University"

authornote: |

abstract: |
  Children process words in complex contexts that lend themselves to an in principle unlimited number of possible interpretations. How do learners find the correct lexicon? Statistical accounts propose that learning unfolds via the aggregation of consistent word-object co-occurrences over time. Social-pragamatic theories emphasize how grounded interactions with social partners reduces ambiguity during individual labeling events. Here, we present three studies of eye movements during language processing that ask how learners intergrate statistical and social information to shape real-time decisions about visual fixation. First, both children (n=XXX) and adults (n=31) showed similar gaze dynamics when processing familiar words that either did or did not occur with an accompanied social cue to reference (eye gaze). Second, in a minimal cross-situational word learning task, adults (n=XXX) allocated more fixations and showed stronger memory for novel word-object mappings that were learned in the presence of a social cue. Finally, in contrast to the familiar word context, both children (n=XXX) and adults (n=XXX) were slower to look away from a speaker's face when she was likely to provide a gaze cue to disambiguate the meaning of a novel word. This differential looking pattern increased over the course of the experiment, as learners were exposed to more word-object co-occurrences and gained  experience with the speaker. Taken together, these results show that decisions about how to seek visual information during language acquisition are a function of the interaction of statistical and social information.
  
keywords          : "eye movements; word learning; language comprehension; information-seeking; gaze following"
wordcount         : "X"
bibliography      : ["speed-acc-novel.bib", "r-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
knitr::opts_chunk$set(fig.pos = '!t', fig.width = 6, fig.asp=0.8, out.width = "90%", 
                      cache = T, fig.path='figs/', fig.align = 'center',
                      echo=F, warning=F, message=F, 
                      sanitize = T)

source(here::here("code/helper_functions/paper_helpers.R"))
bib <- knitcitations::read.bibtex(here::here("writing/manuscript/speed-acc-novel.bib"))

# data_paths
demo_path <- "data/01_participant_logs/"
data_path <- "data/03_processed_data/"
stimuli_path <- "data/00_stimuli_information/analysis_order_sheets"
image_path <- "writing/figures/plots"
```

```{r model-globals}
options (mc.cores=parallel::detectCores()) # Run on multiple cores
set.seed (3875)
```

```{r read stimuli information}
d_gaze_stim <- read_csv(here::here(stimuli_path, "speed-acc-child-gaze-trial-info.csv"), col_types = cols(.default = "c"))
d_adult_ng_stim <- read_csv(here::here(stimuli_path, "speed-acc-adult-ng-trial-info.csv"), col_types = cols(.default = "c"))
d_stim <- bind_rows(mutate(d_gaze_stim, experiment = "kids_gaze"), mutate(d_adult_ng_stim, experiment = "adults_ng"))
```

```{r read_familiar_words_data}
d_fam_child <- read_csv(here::here(data_path, "familiar_words/speed_acc_child_gaze_fstshift_tidy.csv")) %>% 
  rename(age_days = age) %>% 
  mutate(age_group = "children")

d_fam_adult <- read_csv(here::here(data_path, "familiar_words/speed_acc_adult_ng_fstshift_tidy.csv")) %>% 
  filter(noise_condition == "clear") %>% 
  rename(run_date = dot,
         comments = notes) %>% 
  select(-age, -resp_onset_type_fact, -reason_excluded) %>% 
  mutate(birthday = NA,
         age_group = "adults", 
         age_days = NA,
         run_date = lubridate::as_datetime(run_date))

# clean up and join kid and adult data
d_fam <- bind_rows(d_fam_child, d_fam_adult)
```

```{r read_speed_novel_data}
d_novel_fs <- read_csv(here::here(data_path, "novel_words/speed_acc_novel_shifts.csv")) %>% 
  mutate(learn_block_half = ifelse(trial_num_learn_block <= 2, "first", "second"),
         trial_num_lblock_8 = case_when(
           trial_type == "exposure" & trial_num_learn_block == 1 ~ "1",
           trial_type == "test" & trial_num_learn_block == 1 ~ "2",
           trial_type == "exposure" & trial_num_learn_block == 2 ~ "3",
           trial_type == "test" & trial_num_learn_block == 2 ~ "4",
           trial_type == "exposure" & trial_num_learn_block == 3 ~ "5",
           trial_type == "test" & trial_num_learn_block == 3 ~ "6",
           trial_type == "exposure" & trial_num_learn_block == 4 ~ "7",
           trial_type == "test" & trial_num_learn_block == 4 ~ "8",
           TRUE ~ "NA"
         ),
         correct_bool = ifelse(shift_accuracy == "correct", TRUE, FALSE))

d_novel_tc <- read_feather(here::here(data_path, "novel_words/speed_acc_novel_timecourse.feather")) 
```

# Introduction

# Analytic approach

To quantify evidence for our predictions, we follow @macdonald2018speed and present four analyses: (1) the timecourse of listeners' looking to each area of interest (AOI), (2) the Reaction Time (RT) and Accuracy of listeners' first shifts away from the speaker's face, (3) an Exponentially Weighted Moving Average (EWMA), and (4) a Drift Diffusion Model (DDM). ^[All analysis code can be found in the online repository for this project: https://github.com/kemacdonald/speed-acc-novel.]

First, we analyzed the timecourse of participants' looking to each AOI in the visual scene as the target sentence unfolded. Proportion looking reflects the mean proportion of trials on which participants fixated on the speaker, the target image, or the distracter image at every 33-ms interval of the stimulus sentence. We tested condition differences in the proportion looking to the language source -- signer or speaker -- using a nonparametric cluster-based permutation analysis, which accounts for the issue of taking multiple comparisons across many time bins in the timecourse [@maris2007nonparametric]. A higher proportion of looking to the language source in the gaze condition would indicate listeners' prioritization of seeking visual information from the speaker.

Next, we analyzed the RT and Accuracy of participants' initial gaze shifts away from the speaker to objects. RT corresponds to the latency of shifting gaze away from the central stimulus to either object measured from the onset of the target noun. All reaction time distributions were trimmed to between zero and two seconds and RTs were modeled in log space. Accuracy corresponds to whether participants' first gaze shift landed on the target or the distracter object. If listeners generate slower but more accurate gaze shifts, this  provides evidence that gathering more visual information from the signer/speaker led to more robust language comprehension.

We used the `rstanarm` [@gabry2016rstanarm] package to fit Bayesian mixed-effects regression models. The mixed-effects approach allowed us to model the nested structure of our data -- multiple trials for each participant and item, and a within-participants manipulation -- by including random intercepts for each participant and item, and a random slope for each item and gaze condition. We used Bayesian estimation to quantify uncertainty in our point estimates, which we communicate using a 95% Highest Density Interval (HDI). The HDI provides a range of credible values given the data and model.

Following the behavioral results, we present two model-based analyses ^[For more details about the model-based analyses see @macdonald2018speed or Chapter 3.]. The goal of each model is to move beyond a description of the data and to map behavioral differences to underlying psychological processes. The EWMA models changes in the tendency to generate random gaze shifts as a function of RT [@vandekerckhove2007fitting]. This model allows us to quantify the proportion of gaze shifts that were classified as language-driven as opposed to guessing. If listeners seek more visual information from the language source, then they should generate a higher proportion of language-driven shifts and fewer random responses.

Finally, following @vandekerckhove2007fitting, we selected the gaze shifts categorized as language-driven by the EWMA and fit a hierarchical Bayesian Drift-Diffusion Model (HDDM) [@wiecki2013hddm]. The DDM is a cognitive model of decision making developed over the past forty  years (Ratcliff  &  McKoon,  2008) that can quantify differences in the underlying decision process that lead to different patterns of observable behavior. Here, we focus on two parameters of interest: *boundary separation*, which indexes the amount of evidence gathered before generating a response (higher values suggest more information gathered before responding) and *drift rate*, which indexes the amount of evidence accumulated per unit time (higher values suggest more efficient processing). If listeners have a higher boundary separation estimate, this provides additional evidence that more accurate responses were driven by changes in information accumulation as opposed to processing efficiency.

# Experiment 1

In Experiment 1, we measured the timecourse of children and adults' decisions about visual fixation as they processed sentences with familiar words  (e.g., "Where's the ball?"). ^[See https://osf.io/2q4gw/for a pre-registration of the analysis plan.]. We manipulated whether the speaker produced a post-nominal gaze cue to the named object. The visual world consisted of three fixation targets (a center video of a person speaking, a target picture, and a distracter picture; see Figure XXX). The primary question of interest is whether listeners would delay shifting away from the speaker's face when she was likely to generate a gaze cue. We predicted that choosing to fixate longer on the speaker would allow listeners to gather more language-relevant visual information and facilitate comprehension. In contrast, if listeners show parallel gaze dynmamics across the gaze and no-gaze conditions, this pattern suggests that hearing the familiar word was the primary factor driving shifts in visual attention.

## Methods

### Participants

```{r exclusions-table}
n_exclude_et_e1 <- d_fam %>% 
  filter(age_group != "adult") %>% 
  select(subid, keep_et) %>% 
  unique() %>% 
  count(keep_et)

n_exclude_runsheet_e1 <- d_fam %>% 
  filter(age_group != "adult") %>% 
  select(subid, keep_runsheet) %>% 
  unique() %>% 
  count(keep_runsheet)
```

```{r make-ss-table, results="asis"}
d_kids <- d_novel_fs %>% 
  filter(age_category == "child", keep_drop == "keep") %>% 
  select(subid, age_days, gender) %>% 
  mutate(Experiment = "Experiment 3 (novel words)") %>% 
  unique()

d_kids %<>% bind_rows(
  d_fam %>% 
    filter(age_group != "adult", keep_et == "include", keep_runsheet == "keep") %>% 
    select(subid, age_days, gender) %>% 
    unique() %>% 
    mutate(Experiment = "Experiment 1 (familiar words)" )
)

gender_breakdown <- d_kids %>% 
  count(Experiment, gender)

kids_age_table <- d_kids %>% 
  mutate(age_months = age_days / 30.25,
         age_years = age_days / 365.25) %>% 
  group_by(Experiment) %>% 
  summarise(n = n(),
            Mean = round(mean(age_months, na.rm = T), 1),
            Min = min(age_months, na.rm = T),
            Max = max(age_months, na.rm = T))

kids_age_table[, -1] <- printnum(kids_age_table[, -1])

apa_table(
  kids_age_table
  , caption = "Age distributions of children in Experiments 1 and 3. All ages are reported in months."
)
```

```{r adults-demo}
n_adults_gender <- d_novel_fs %>% 
  filter(age_category == "adult", keep_drop == "keep") %>% 
  select(subid, gender) %>% 
  unique() %>% 
  count(gender) %>% 
  mutate(Experiment = "Experiment 3")

n_adults_gender %<>% bind_rows(
  d_fam %>% 
    filter(age_group == "adult", keep_et == "include") %>% 
    select(subid, gender) %>% 
    unique() %>% 
    count(gender) %>% 
    mutate(Experiment = "Experiment 1")
)

n_adults_total <- n_adults_gender %>% group_by(Experiment) %>% summarise(n = sum(n))
```

Participants were native, monolingual English-learning children ($n=$ `r kids_age_table$n[1]`; `r gender_breakdown$n[1]` F) and adults ($n=$ `r n_adults_total$n[1]`; `r n_adults_gender$n[3]` F). All participants had no reported history of developmental or language delay and normal vision. XXX participants (XXX children, XXX adults) were run but not included in the analysis because either the eye tracker falied to calibrate (XXX children, XXX adults) or the participant did not complete the task (XXX children). 

### Materials

```{r linguistic stimuli length}
d_word_lengths <- d_stim %>% 
  filter(!is.na(noun)) %>% 
  select(noun, carrier, noun_onset_sec:noun_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("noun_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (noun_offset_sec * 33) + noun_offset_frames ) - ( (noun_onset_sec * 33) + (noun_onset_frames) ),
    length_ms = length_frames * 33
  ) 

ms_words_length <- d_word_lengths %>% 
  group_by(carrier, noun) %>% 
  summarise(m_length = mean(length_ms)) %>% 
  group_by(noun) %>% 
  summarise(m = mean(m_length) %>% round(digits = 2)) 

## compute gaze length
d_gaze_length <- d_stim %>% 
  filter(!is.na(gaze_onset_sec)) %>% 
  select(noun, carrier, gaze_onset_sec:gaze_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("gaze_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (gaze_offset_sec * 33) + gaze_offset_frames ) - ( (gaze_onset_sec * 33) + (gaze_onset_frames) ),
    gaze_length_ms = length_frames * 33
  ) 

ms_gaze_length <- d_gaze_length %>% 
  summarise(m_gaze = mean(gaze_length_ms) / 1000)

```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (target word)" or "Look! Where's the (target word). The target words were: ball, bunny, boat, bottle, cookie, juice, chicken, and shoe. The target words varied in length (shortest = `r min(ms_words_length$m)` ms, longest = `r max(ms_words_length$m)` ms) with an average length of `r mean(ms_words_length$m) %>% round(digits=2)` ms. 

*Gaze manipulation*. To create the stimuli in the gaze condition, the speaker waited until she finished producing the target sentence and then turned her head to gaze at the bottom right corner of the camera frame. After looking at the named object, she then returned her gaze to the center of the frame. We chose to allow the length of the gaze cue to vary to keep the stimuli naturalistic. The average length of gaze was `r printnum(ms_gaze_length$m_gaze)` seconds with a range from `r printnum(min(d_gaze_length$gaze_length_ms) / 1000)` to `r printnum(max(d_gaze_length$gaze_length_ms) / 1000)` seconds.

*Visual stimuli.* The image set consisted of colorful digitized pictures of objects presented in fixed pairs with no phonological overlap between the target and the distracter image (cookie-bottle, boat-juice, bunny-chicken, shoe-ball). The side of the target picture was counterbalanced across trials.

### Procedure

Participants viewed the task on a screen while their gaze was tracked using an SMI RED corneal-reflection eye-tracker mounted on an LCD monitor, sampling at 30 Hz. The eye-tracker was first calibrated for each participant using a 6-point calibration. On each trial, participants saw two images of familiar objects on the screen for two seconds before the center stimulus appeared. Next, they processed the target sentence -- which consisted of a carrier phrase, a target noun, and a question -- followed by two seconds without language to allow for a response. Both children and adults saw 32 trials (16 gaze trials; 16 no-gaze trials) with several filler trials interspersed to maintain interest. The gaze manipulation was presented in a blocked design with the order of block counterbalanced across participants.

```{r gaze-stimuli, fig.cap = "Stimuli for Experiments 1 and 3. Panel A shows the timecourse of the linguistic stimuli for a single trial. Panel B shows the layout of the fixation locations for all tasks: the center stimulus, the target, and the distracter. Panel C shows a sample of the images used as novel objects in Experiment 3. Panel D shows the social gaze manipulation."}

knitr::include_graphics(here::here(image_path, "gaze_stimuli.jpeg"))
```


## Results and Discussion

```{r speed-acc-gaze-results, out.width="95%", fig.cap = "Timecourse looking, first shift Reaction Time (RT), and Accuracy results for children in Experiment 1. Panel A shows the overall looking to the center, target, and distracter stimulus for each context. Panel B shows the distribution of pairwise contrasts between RTs in the gaze and no-gaze conditions. The square point represents the mean value for each measure. The vertical dashed line represents the null model of zero condition difference. The width each point represents the 95\\% HDI. Panel C shows the same information but for participants' first shift accuracy."}

knitr::include_graphics(here::here(image_path, "speed_acc_fam_behav.jpeg"))
```

*Timecourse.*

```{r}
d_model_fam <- d_fam %>% 
  filter(keep_runsheet %in% c("yes", "keep"), 
         keep_et == "include",
         rt <= 2,
         response_onset_type == "noun",
         shift_start_location == "center")
```


```{r blmm speed-acc-fam}
# RT
fit_rt_fam <- brms::brm(log(rt) ~ gaze_condition + age_group + (gaze_condition + target_image|subid), 
                        data = d_model_fam, 
                        family = gaussian(),
                        silent = TRUE)

coefs_san_rt <- broom::tidy(fit_rt_fam)

# accuracy
```

*First shift RT and Accuracy.*

# Experiment 2

```{r read_gaze_xsit_data}
d_gz_xsit <- read_csv(here::here(data_path, "novel_words/gaze_xsit_tidy_trial_level.csv"))
d_gz_xsit_tc <- read_csv(here::here(data_path, "novel_words/gaze_xsit_tidy_timecourse.csv"))
```

Because children hear language in environments with multiple possible referents, learning the meaning of even the simplest word requires reducing this uncertainty. A cross-situational statistical learner can aggregate across ambiguous naming events to learn stable word meanings. But for this aggregation process to work, learners must allocate their limited attention and memory resources to the relevant statistics in the world – how do they select what information to store? 

In prior work (discussed in Chapter 4), we found that the presence of a gaze cue shifted adults away from storing multiple word-object links and towards tracking a single hypothesis. Those experiments, however, relied on an offline measurement of word learning (a button press on test trials) and an indirect measure of attention during learning (self-paced decisions about how long to inspect the visual scene during learning trials). We begain to address these limitations in a pilot study where we adapted the social cross-situational learning paradigm to use eye-tracking methods. Moving to an eye-tracking procedure allowed us to ask (1) how does the presence of gaze alter the distribution of visual attention during labeling? and (2) does the presence of a gaze cue change the strength of learners' inferences about word-object links? 

## Methods

### Participants

34 undergraduate students were recruited from the Stanford Psychology One credit pool (17 F). Four participants were excluded during analysis because the eye-tracker did not properly record their gaze coordinates. The final sample included 30 participants.

### Materials

The experiment featured sixteen pseudo-words recorded by an AT&T Natural VoicesTM speech synthesizer using the "Crystal"" voice (a woman’s voice with an American English accent), as well as 48 novel objects represented by black-and-white drawings of fictional objects from Kanwisher, Woods, Iacoboni, and Mazziotta (1997). Sixteen words were used so that the experiment would be sufficiently long to make within-subject comparisons across trials, and 48 objects were used so that objects would not be repeated across trials. Six familiar objects from the same set of drawings were used for the two practice trials, accompanied by two familiar words using the same speech synthesizer. Finally, the videos of the speaker’s face were taken from from @macdonald2017social. 

### Procedure

We tracked adults' eye movements while they watched a series of ambiguous word-learning events (16 novel words) organized into pairs of exposure and test trials (32 trials total). All trials consisted of a set of two novel objects and one novel word. Participants were randomly assigned to either the Gaze condition in which a speaker looked at one of the objects on exposure trials or the No-Gaze condition in which a speaker looked straight on exposure trials. Every exposure trial was followed by a test trial, where participants heard the same novel word paired with a new set of two novel objects. One of the objects in the set had appeared in the exposure trial ("kept" object), while the other object had not previously appeared in the experiment ("novel" object). 

The side of the screen of the "kept" object was counterbalanced throughout the experiment. In the gaze condition, for half of the test trials, the kept object was the target of the speaker’s gaze during the exposure trial, while the other half, the kept object was the object that had not been the gaze target.

## Results and Discussion

```{r gaze-xsit-tc-plot, fig.width=3, out.width="100%", fig.cap = "Overview of adults' looking to the three fixation targets (Face, Target, Distracter) over the course of the trial. Panel A shows proportion looking to the speaker's face for exposure and test trials. Color and linetype represent gaze condition. Panel B shows the same information but for proportion looking to the target and distracter images."}

knitr::include_graphics(here::here(image_path, "gaze_xsit_tc.jpeg"))
```

*Timecourse looking.* The first question of interest was how did the presence of a gaze cue change adults' distribution of attention across the three fixation locations while processing language in real-time? Figure\ \ref{fig:gaze-xsit-tc-plot} presents an overview of looking to each AOI for each processing context. This plot shows changes in the mean proportion of trials on which participants fixated on the speaker's face, the target image, or the distracter image at every 33-ms interval of the stimulus sentence. 

At target-noun onset, adults tended to look more at the speaker's face on both exposure and test trials. As the target noun unfolded, the mean proportion looking to the center decreased as participants shifted their gaze to the target or the distracter images. On exposure trials tended to distribute their attention relatively evenly across target and distracter images. On test trials, proportion looking to the target increased sooner and reached a higher asymptote compared to proportion looking to the distracter for both condition, suggesting that adults were able to track the consistent word-object links in both conditions.

There were several qualitative differences in looking behavior across the different gaze conditions and trial types. First, adults spent more time looking to a speaker's face when she provided a social gaze cue, especially on test trials that were preceded by gaze (Panel A of Figure XXX). Second, adults in the gaze condition looked slightly more to the target image over the course of the trial. This behavior is reasonable since half of the trials, the speaker's gaze was focused on the target image that would appear on the subsequent test trial. Third, on test trials, adults looked more to the images in the no-gaze context. This led to a higher proportion of looking to the target, but also a higher proportion of looking to the distracter images (Panel B of Figure XXX). 

Together, these looking patterns provide suggestive evidence that the presence of a gaze cue caused adults to spend more time gathering visual information from the speaker's face. Next, we ask how the social gaze cue modulated learning, which we operationalized as the relationship between looking behavior on exposure and test trials.

```{r gaze-xsit-prop-looking-plot, fig.cap = "Panel A shows participants’ tendency to look at the object that was the target of the speaker’s gaze on exposure trials. The vertical, dashed line represents the mean proportion of time looking to the gaze target across all trials. Panel B shows the relationship between adults' looking behavior on exposure and test trials for the gaze and no-gaze conditions."}

knitr::include_graphics(here::here(image_path, "gaze_xsit_prop_looking.jpeg"))
```

```{r gaze-xsit-model, include = FALSE}
ss_gz_xsit <- d_gz_xsit %>% 
  filter(!is.na(prop_looking)) %>% 
  select(subid, trial_type, condition, m_correct, trial_num) %>% 
  distinct() %>% 
  spread(trial_type, m_correct)

fit_gaze_xsit <- brms::brm(test ~ exposure * condition + (1|subid), 
            data = ss_gz_xsit, family = gaussian(),
            silent = TRUE)

coefs_gaze_xsit <- broom::tidy(fit_gaze_xsit)
```

*Relationship between performance on exposure and test trials.* When the speaker generated a social cue during labeling, adults reliably followed that cue and tended to focus their attention on a single object (Figure XXXA). In contrast, people in the No-gaze condition tended to distribute their attention more broadly across the two objects. For adults in both gaze contexts, more time spent attending to the target object on exposure trials led higher proportion looking to the target, i.e., better recall, at test ($\beta_{exposure}$ = `r printnum(coefs_gaze_xsit$estimate[2])`, 95% HDI [`r printnum(coefs_gaze_xsit$lower[2])`, `r printnum(coefs_gaze_xsit$upper[2])`]). Critically, there was an interaction between the gaze condition and the relationship between attention on exposure and test: When eye gaze cued visual attention, adults showed stronger memory for the word-object link ($\beta_{int}$ = `r printnum(coefs_gaze_xsit$estimate[4])`, 95% HDI [`r printnum(coefs_gaze_xsit$lower[4])`, `r printnum(coefs_gaze_xsit$upper[4])`]). This result provides evidence that social information does not only modulate people's in-the-moment decisions about how to allocate their visual attention; instead, the social cue changed the strength of the memory for word-object links that are stored during cross-situational word learning.

### Limitations

There were several key limitations of our pilot study. First, we chose to start the liguistic stimulus as soon as the images and the speaker appeared on the screen (i.e., at trial onset). This made it difficult to analyze the timing and accuracy of first shifts decisions away from the speaker and to the objects. Second, this trial structure did not allow us to measure decisions about visual fixation that occur before the start of language comprehension while learners are first gathering information about the visual world. Finally, the linguistic stimuli consisted of sixteen pseudowords recorded by a speech synthesizer and presented in isolation, thus removing any sentential context. Presenting isolated words is unlikely to work with younger age groups and does not allow us to separate decisions about fixations made during language processing more broadly from decisions that occur after the onset of the target noun -- a critical distinction for our modeling of the underlying decision-making process. 

# Experiment 3

Experiment 3 was designed to ask how social information modulates learners' real-time visual information selection as they accumulate knowledge about novel word-object links ^[See https://osf.io/nfz85/ for a pre-registration of the analysis plan and predictions.]. We also set out to address the limitations of Experiment 2 discussed above. There were two key modifications. First, we modified the cross-situational learning paradigm to include more than two exposures to a novel word-object link. This allowed us to measure changes in learners' integration of social and statistical information over a longer timescale. Second, we changed the linguistic stimuli to parallel the design of Experiment 1 such that the novel words occurred within a target sentence. This allowed us to leverage the analytic techniques developed for analyzing participants' initial gaze shifts in Experiment 1 to ask how decisions about visual information gathering from social partners changed as a function of repeated exposure to statistical information about word-object mappings.

\noindent
We aimed to answer the following specific research questions: 

  1. How do decisions about where to allocate visual attention (speakers vs. objects) change as a function of learning a new word? 
  2. How does the presence of a social cue to reference (eye gaze) change the dynamics of children's gaze patterns during object labeling? 
  3. What is the relationship between chidren's gaze patterns during object labeling and their memory of new words?
  
To answer these questions, we compared the timing and accuracy of eye movements during a real-time cross-situational learning task where participants process sentences that contain a novel word (e.g., "Where's the *dax*?") while looking at a simplified visual world with 3 fixation targets (a video of a speaker and two images of unfamiliar objects). We had three key behavioral predictions. First, for all conditions, participants’ distribution of attention to speakers compared to objects will shift over the course of learning. Early in the task, participants will allocate more fixations to a speaker to prioritize gathering visual information that disambiguates reference. After experiencing multiple exposures to a word-object pairing, participants will generate faster saccades, showing signatures of comprehension of the incoming speech. We further predict that later in learning blocks, participants will allocate more fixations to the objects, showing looking behaviors that support learning long-term associations between words and objects.

Second, the presence of a gaze cue will change participants’ decisions about visual fixation. We hypothesize that a post-nominal gaze cue increases the value of fixating on a speaker. This manipulation will cause participants to allocate more fixations to the speaker when gaze is present, leading to slower first shift reaction times and higher proportion looking, especially earlier in learning (i.e., lower trial numbers within each block of exposure trials to a novel word-object pairing). This prediction will be operationalized as a main effect of Gaze condition on RT, and a trial number by Gaze condition interaction such that the decrease in RT will be greater on exposure trials in the Gaze condition.

Third, the Gaze condition should lead to stronger inferences about the correct word-object mapping, resulting in faster learning that we operationalize as more accurate first shifts, faster RTs, and a higher proportion looking to the target vs. the distractor object on test trials as compared to learning words without a gaze cue across both exposure and test trials.

## Methods

### Participants

```{r exclusions-e3}
d_excluded_e3 <- d_novel_fs %>% 
  select(subid, keep_drop, reason_excluded, age_category) %>% 
  unique() %>% 
  filter(keep_drop == "drop") %>% 
  count(reason_excluded, age_category)
```

Participants were native, monolingual English-learning children ($n=$ `r kids_age_table$n[2]`; `r gender_breakdown$n[3]` F) and adults ($n=$ `r n_adults_total$n[2]`; `r n_adults_gender$n[1]` F). All participants had no reported history of developmental or language delay and normal vision. `r d_excluded_e3$n[1]` adults were run but not included in the analysis because they were not native speakers of English. XXX participants were run but not included in analysis because either the eye tracker falied to calibrate (XXX children, XXX adults) or the participant did not complete the task (XXX children). 

### Materials

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (novel word)" or "Look! Where's the (novel word). The target words were four pseudo-words: bosa, modi, toma, and pifo. The novel words varied in length (shortest = XXX ms, longest = XXX ms) with an average length of XXX ms. 

*Gaze manipulation*. To create the stimuli in the gaze condition, the speaker waited until she finished producing the novel word before turning her head to gaze at the bottom right corner of the frame. After looking at the named object, she then returned her gaze to the center of the frame. We chose to allow the length of the gaze cue to vary to keep the stimuli naturalistic. The average length of gaze was `r printnum(ms_gaze_length$m_gaze)` seconds with a range from `r printnum(min(d_gaze_length$gaze_length_ms) / 1000)` to `r printnum(max(d_gaze_length$gaze_length_ms) / 1000)` seconds.

*Visual stimuli.* The image set consisted of 28 colorful digitized pictures of objects that were selected such that children were unlikely to already have a label associated with them. The side of the target picture was counterbalanced across trials.

### Procedure

Participants viewed the task on a screen while their gaze was tracked using an SMI RED corneal-reflection eye-tracker mounted on an LCD monitor, sampling at 30 Hz. The eye-tracker was first calibrated for each participant using a 6-point calibration. Then, participants watched a series of ambiguous word learning events organized into pairs of one exposure and one test trial. On each trial, participants saw of a set of two unfamiliar objects and heard one novel word. 

Each word was learned in a block of four exposure-test pairs for a total of eight trials for each novel word. Critically, on each trial within a word block, one of the objects in the set had appeared on the previous trials (target object), while the other object was a randomly generated novel object not previously shown in the experiment (distracter object). Both children and adults saw 32 trials (16 gaze trials; 16 no-gaze trials) with several filler trials interspersed to maintain interest. The gaze manipulation was presented in a blocked design with the order of block counterbalanced across participants.

## Results and Discussion

### Timecourse looking

```{r san-tc-plot, fig.width=3, out.width="100%", fig.cap = "Overview of children and adults' looking to the three fixation targets (Speaker, Target, Distracter) over the course of exposure and test trials. Panel A shows proportion looking to the speaker's face with color indicating gaze condition and linetype indicating age group. Panel B shows the same information but for proportion looking to the target and distracter images."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_tc.jpeg"))
```

*Learning effects.*

```{r san-prop-looking-plot, fig.cap = "Panel A shows participants’ tendency to look at the target object on exposure and test trials as a function of the trial number within a learning block. The horizontal, dashed line represents the tendency to distribute attention equally across the two images. Color indicates gaze condition and error bars represent 95\\% credible intervals. Panel B shows the same information but collapsed across trial to highlight the effect of the gaze cue."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_proplook.jpeg"))
```

*Looking to the speaker.*

```{r san-prop-looking-speaker-plot, fig.cap = "Children and adults' looking to the speaker as function of trial number within a learning block. All plotting conventions are the same as in Figure XXX."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_speaker_look.jpeg"))
```

### First shift RT and Accuracy

*Effect of gaze cue.*

```{r speed-acc-novel-shifts, out.width="90%", fig.cap = "First shift Reaction Time (RT), and Accuracy results for children and adults in Experiment 3. Panel A shows the distribution of pairwise contrasts between RTs in the gaze and no-gaze conditions. The square point represents the mean value for each measure. The vertical dashed line represents the null model of zero condition difference. The width each point represents the 95\\% HDI. Panel B shows the same information but for participants' first shift accuracy."}

knitr::include_graphics(here::here(image_path, "speed_acc_novel_fstshifts.jpeg"))
```

# General Discussion

\newpage

# References

```{r create_r-references}
my_citations <- cite_r(file = here::here("writing/manuscript/r-references.bib"), 
                       pkgs = c("tidyverse", "rstanarm", "papaja", "here", "knitr"), 
                       withhold = FALSE,
                       footnote = TRUE)
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
